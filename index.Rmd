---
title: "Analyzing Goodreads' Readership"
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
library("tidyverse")
library("snakecase")
library("ggfortify")
library("cluster")
```

## Introduction

[Goodreads](https://www.goodreads.com) is a popular social cataloging website where users can search, track, and rate books. The site has
over 20 million users and 50 million book reviews, making it a very large source of readership data.

![Goodreads site](./images/screenshot_01.png)


With so many users and reviews, an interesting experiment would be to see if
we can characterize the readership of Goodreads using user ratings.
Specifically we are interested in answering the question,
"Based on user ratings data, can we find common groups of users by how they rate certain genres of books?"
With such information, we would be able to see if, for instance, there is
a large group of sci-fi fans or a large group of romance and YA fanatics that
also happen to hate horror.

> "Based on user ratings data, can we find common groups of users by how they rate certain genres of books?"

In this project, we demonstrate the process of doing this kind of analysis from the ground up by leveraging and manipulating
a subset of Goodreads' data using the Goodbooks-10k dataset [@goodbooks2017] and performing
statistical and cluster analysis to try to try to paint a generalized picture of the site's readership.

## Understanding and Pre-processing the Data

The Goodbooks-10k data obtained from GitHub consists of several CSV files with information about the top 10,000 most rated
books on Goodreads, including book metadata, tags, and relevant user ratings.

The CSV files can be easily loaded using R's `read.csv` method. For example, here we load and display
`books.csv` to get a broad picture of one of the datasets we will be working
with. We can see that `books.csv` organizes its data in a tabulated manner with each book as a row
and each book attribute as a column.

```{r}
books_df <- read.csv("./goodbooks-10k/books.csv") %>%
  as_tibble()
head(books_df) %>%
  rmarkdown::paged_table()
```

Let's load the rest of our data as well.

```{r}
ratings_df <- read.csv("./goodbooks-10k/ratings.csv")%>%
  as_tibble()
tags_df <- read.csv("./goodbooks-10k/tags.csv")%>%
  as_tibble()
book_tags_df <- read.csv("./goodbooks-10k/book_tags.csv")%>%
  as_tibble()
```

With our data loaded, we can begin to do some data manipulation. Since we are interested
in analyzing various genres, let's start by narrowing down `tags_df` with
the genre tags we are interested in. We can do this using a **pipeline**
as shown below.

```{r}
tags_df <- tags_df %>%
  filter(tag_name %in% c(
    "fantasy",
    "mystery",
    "horror",
    "realistic-fiction",
    "romance",
    "science-fiction",
    "thriller",
    "young-adult"
  )) %>%
  as_tibble()

tags_df %>%
  rmarkdown::paged_table()
```

However, this tag information is not very helpful by itself. We
want to be able to use it in conjunction with the other data
we have. One way to accomplish this is by **joining** data across
different data frames through a common attribute. We construct a pipeline
to link data from `books_df` and `book_tags_df` through the shared
attribute, `goodreads_book_id`. From there, we then link `book_tags_df` and `tags_df`
through `tag_id`.

```{r}
book_genres_df <- books_df %>%
  inner_join(book_tags_df, by="goodreads_book_id") %>%
  inner_join(tags_df, by="tag_id") %>%
  select("book_id", "title", "tag_name")

book_genres_df %>%
  select("title", "tag_name") %>%
  head(15) %>%
  rmarkdown::paged_table()
```

From the resulting data frame, we can see that `tag_name` does indeed describe
the genre of books it is associated with.

Let's build another pipeline to join with `ratings_df` and link users
with the genres of the books they rated. We can do a summary on the data frame
from there to end up with a table of mean ratings in each genre of interest
for all users. These ratings range from 1 to 5, with a mean rating of -5 to mark
genres with no rating.

```{r}
# Note: This takes awhile to run...

user_features_raw_df <- ratings_df %>%
  inner_join(book_genres_df, by="book_id") %>%
  spread(tag_name, rating) %>%
  select(-book_id, -title,)

user_features_df <- user_features_raw_df %>%
  group_by(user_id) %>%
  summarize_all(mean, na.rm=TRUE) %>%
  select(-user_id,) %>%
  mutate_if(~ any(is.na(.x)),~ if_else(is.na(.x),-5,.x)) # Replace na with -5

# Convert column names to snake_case
colnames(user_features_df) = to_snake_case(colnames(user_features_df))

head(user_features_df, 15) %>%
  mutate_all(round, 2) %>%
  rmarkdown::paged_table()
```

## Visualization and Analysis

Now that we have transformed the original data into a data frame of average
genre ratings for all users, we can begin analyzing users
based on genre ratings.

One way to better understand data in general is through visualization.
For example, if we want to see where the readership lies on
ratings for young adult versus fantasy, we can do so by plotting the average ratings
from both genres on a 2D scatterplot.

```{r, fig.align='center'}
ggplot(user_features_df, mapping=aes(x=fantasy, y=young_adult)) +
  geom_point() +
  labs(
    title="User Average Ratings for Young Adult Versus Fantasy",
    x="Fantasy Rating",
    y="Young Adult Ratings"
  )
```

From the
graph, it appears most user have average ratings clustered between 3 and 5 with no
strong preference for one genre versus the other. There are
also several users that appear to be readers of one
genre but not the other, and at least one user that
does not read from either.

### Clustering for Higher Dimensions

While a 2D graph is fine for comparing two genres, it becomes harder
to visualize data like this beyond 3 variables since we run out of spatial dimensions.
Furthermore, we are also interested in clustering our data. While we had done a very cursory
grouping by eye-balling the data above, ideally, there should be a way to more formally
say which data points form a group with one another.

The solution to both of these problems is found in two methods known as **K-means clustering**
and **principal component analysis (PCA)**.

K-means clustering is a way to cluster, or group, data points. The algorithm designates
K centroids representing the centers of their respective clusters. Each data point is assigned
to the closest centroid, and the algorithm iteratively optimizes the centroids try to reduce the total distance
between it and all the points in its cluster.

![K-means on iris flower species](images/kmeans_iris.png)

Meanwhile, principal component analysis (PCA) is a way to reduce the dimensions in high-dimensional data while
still trying to preserve features from the original space. Roughly, PCA tries to find a set of linear
transformations from the original data that prioritizes maximizing variance for each dimension of the transformed data.
At the end, the first two dimensions of the transformed data will ideally encode enough of the variance from the original
high-dimensional data that graphing only those two dimensions should suffice in representing the original data, making PCA a very
a useful tool for visualization.

![Example PCA: The transformed coordinate system in red maximizes the variance of the pixel locations.](images/pcafish.png)

When used in conjunction, K-means clustering will allow us to formally cluster
multi-dimensional data and PCA will allow us graph the clustered data in 2D for easy
visualization.

### Determining K

A key roadblock with using K-means is that we do not have a good idea of what is a "good" K, or number of clusters. One way we can
address this is by doing several test runs with different K's and comparing them, specifically,
by comparing their total **sum of squared distances (SSE)**. Intuitively, a good clustering should
minimize the distances of points to the centroids of their respective clusters which would be reflected
in a low total SSE [@kmeansTDS].

While we want to lower variation within clusters, at the same time, we also want our clusters to actually be
clusters. This means that for any data point, it should ideally be close to the centroid of its cluster
and far from others, a metric known as the **silhouette** [@kmeansUCR]. While increasing K to an
unreasonably large number may reduce total SSE, data in such a clustering would have low silhouettes since data points in one
cluster would be very close to data points in other clusters.

The goal of finding a good K then resides in reducing SSE while still trying to keep silhouette high. Let's
compute and graph both SSE and silhouette from a sample of the users to see what a good K might be.

```{r, fig.align='center'}
set.seed(320)

k_range <- 2:20
user_features_sample_df <- sample_n(user_features_df, 1000)

# SSE
sse <- sapply(k_range, function(k) {
  kmeans(user_features_sample_df, k)$tot.withinss
})

ggplot(mapping=aes(x=k_range, y=sse)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title="Total SSE vs K",
    x="k",
    y="Total SSE"
  )

# Average silhouettes
sil <- sapply(k_range, function(k) {
  km.res <- kmeans(user_features_sample_df, centers=k)
  ss <- silhouette(km.res$cluster, dist(user_features_sample_df))
  mean(ss[,3])
})

ggplot(mapping=aes(x=k_range, y=sil)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title="Average Silhouettes vs K",
    x="k",
    y="Average Silhouette"
  )
```

Observing both graphs, K = 6 appears to be a point where total SSE has begun to flatten but
average silhouette is still somewhat high, making it a reasonable compromise for K. With K
in hand, we can run K-means clustering on the full data now.

```{r, fig.align='center'}
set.seed(320)

kmeans_res <- kmeans(user_features_df, 6)
kmeans_res$centers %>%
  as_tibble() %>%
  mutate_all(round, 2) %>%
  rmarkdown::paged_table()

# Autoplot automatically does PCA
autoplot(kmeans_res, data=user_features_df, frame=TRUE) +
  labs(title="Readership Clusters")
```

## Making Predictions

```{r}
# TODO: cluster prediction (logistic regression? decision tree?)

# PCA
pca_res <- prcomp(data, center=TRUE, scale.=TRUE)
plot_data <- pca_res$x[,1:2] %>%
  as_tibble() %>%
  mutate(cluster=factor(kmeans_res$cluster))

ggplot(plot_data, aes(PC1, PC2, color=cluster)) +
  geom_point()

# Set up training data for cluster prediction
training_data <- user_features_df %>%
  mutate(cluster=kmeans_res$cluster)

head(training_data, 15)
```

## Results and Discussion

Let's try to qualitatively summarize our clusters:

  1. Rates average 4 in all genres but doesn't read realistic fiction
  2. Rates average 4 in all genres
  3. Rates average 3-4 in all genres but dislikes SF and doesn't read horror
  4. Rates average 4.5-5 in all genres
  5. Rates average 3.5 in all genres 
  6. Rates average 3 all genres

Observing the graph of clusters, groups 2, 4, 5, and 6 appear to be located
close to each other. Among the clusters, groups 1, 3, and 6 show the
largest amounts of variation within their clusters while groups
2, 4, and 5 are tightly clustered.

The close locations of groups 2, 4, 5, and 6 mean that the user
ratings in these groups are harder to distinguish from each other which
makes sense as these are all groups that are similar to each other in that they have
users who have constant average ratings across all genres.

Observing group 6, both its large variation
and slight intersection into group 5 seem to indicate that while the 
average ratings are around 3, there are probably a substantial amount of users
within the group that rate very low, dragging the averages down.

Group 3 is also an interesting case. The cluster has large variation and a
somewhat specific qualitative description. Moreover, informal observation of the
cluster graph seems to show smaller clusters within this group as well. This seems to indicate
that while the users in this group generally have the qualitative characteristics as
described above in common, there are still sub-groups within this cluster
that have varying defining characteristics from each other.

Group 1 is another cluster with a specific qualitative description. The variation
within this cluster is somewhat high and the cluster is located far away from
other the other groups. This seems to indicate that there truly is a common group
of readers that do not read realistic fiction, but they also likely vary in their average
ratings across other genres.

**TODO: Write up predictions**

While we were able to successfully cluster users in this project, there
are still several potential areas for improvement. One issue was that by only
working with the average genre ratings for each user, we lost
information about the variance of ratings within each genre per user. This means that
certain users may have ended up in the same cluster because they had similar average ratings
but still rate books differently in reality because the variation in how they rate was not accounted for.

Another area that could have been improved was our clustering algorithm.
K-means clustering was not able to find some of the smaller clusters in group 6,
possibly due to a large amount of users in other locations which ended up occupying the algorithm.
Since there were more data points densely packed in the area near groups 2 and 5, the centroids
for K-means ended up gathering near there and ignoring smaller clusters elsewhere.

**TODO: Talk more on improvements**

## Conclusion

In this project, we demonstrated how data from Goodreads could be used to find
common groups of users among the site's readership. We were able to do this through
a combination of data manipulation with pipelines and statistical analyses with K-means
clustering and PCA.

These clusterings revealed some insights, such as the existence of a large group of users
that do not read realistic fiction and a large group of users that have little interest in SF and
horror. While our clustering was successful, there are still several areas of improvement we could
have addressed for characterizing users, such as accounting for rating variation
per user and the use of more advanced clustering algorithms.

Clustering a site's userbase is a problem that extends beyond Goodreads. In general, being able to find
natural clusterings of users is both useful and powerful in that it allows for a large number of people
to be bucketed into human-understandable categories. Knowing these clusters can let platforms
gain a better sense of their userbase and allow them to better target content and appropriately address their communities.

## References

<div class="pixel"></div>
<div class="holo"></div>
