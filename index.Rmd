---
title: "Analyzing Goodreads' Readership"
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
library("tidyverse")
library("broom")
library("snakecase")
library("ggfortify")
library("cluster")
library("randomForest")	
library("e1071")	
library("caret")
```

## Introduction

[Goodreads](https://www.goodreads.com) is a popular social cataloging website where users can search, track, 
and rate books. The site also offers a variety of recommendation services that provide users a way to find 
books relevant to their reading history. Since its foundation in 2006, the site's userbase has grown to 20 million 
users with over 50 million book reviews, making it a very large [source of readership data](https://github.com/zygmuntz/goodbooks-10k).

![Goodreads site](./images/screenshot_01.png)


With so many users and reviews, an interesting experiment would be to see if
we can characterize the readership of Goodreads based on user ratings.
Specifically we are interested in answering the question,
"Based on user ratings data, can we find common groups of users by how they rate certain genres of books?"
With such information, we would be able to see if, for instance, there is
a large group of sci-fi fans or a large group of romance and young adult fanatics that
also happen to hate horror.

> "Based on user ratings data, can we find common groups of users by how they rate certain genres of books?"

In this project, we demonstrate the process of doing this kind of analysis from the ground up by leveraging and manipulating
a subset of Goodreads' data using the [Goodbooks-10k dataset](https://github.com/zygmuntz/goodbooks-10k) [@goodbooks2017] and performing
statistical and [cluster analysis](https://www.statisticssolutions.com/directory-of-statistical-analyses-cluster-analysis/) to try to try 
to paint a generalized picture of the site's readership.

## Understanding and Pre-processing the Data

The Goodbooks-10k data obtained from [GitHub](https://github.com/zygmuntz/goodbooks-10k) consists of several CSV files with information about the top 10,000 most rated
books on Goodreads, including book metadata, tags, and relevant user ratings.

The CSV files can be easily loaded using R's `read.csv` method. For example, here we load and display
`books.csv` to get a broad picture of one of the datasets we will be working
with. We can see that `books.csv` organizes its data in a tabulated manner with each book as a row
and each book attribute as a column.

```{r}
books_df <- read.csv("./goodbooks-10k/books.csv") %>%
  as_tibble()
head(books_df) %>%
  rmarkdown::paged_table()
```

Let's load the rest of our data as well.

```{r}
ratings_df <- read.csv("./goodbooks-10k/ratings.csv")%>%
  as_tibble()
tags_df <- read.csv("./goodbooks-10k/tags.csv")%>%
  as_tibble()
book_tags_df <- read.csv("./goodbooks-10k/book_tags.csv")%>%
  as_tibble()
```

With our data loaded, we can begin to do some data manipulation. Since we are interested
in analyzing various genres, let's start by narrowing down the tags dataframe, `tags_df`, with
the genre tags we are interested in. We can do this using a [**pipeline**](https://cfss.uchicago.edu/notes/pipes/)
as shown below.

```{r}
tags_df <- tags_df %>%
  filter(tag_name %in% c(
    "fantasy",
    "mystery",
    "horror",
    "realistic-fiction",
    "romance",
    "science-fiction",
    "thriller",
    "young-adult"
  )) %>%
  as_tibble()

tags_df %>%
  rmarkdown::paged_table()
```

However, this tag information is not very helpful by itself. We
want to be able to use it in conjunction with the other data
we have. One way to accomplish this is by [**joining**](http://www.datasciencemadesimple.com/join-in-r-merge-in-r/) data across
different data frames through a common attribute. We construct a pipeline
to link data from `books_df` and `book_tags_df` through the shared
attribute, `goodreads_book_id`. From there, we then link `book_tags_df` and `tags_df`
through `tag_id`, creating a new dataframe with an entry for each book's genre tag.

```{r}
book_genres_df <- books_df %>%
  inner_join(book_tags_df, by="goodreads_book_id") %>%
  inner_join(tags_df, by="tag_id") %>%
  select("book_id", "title", "tag_name")

book_genres_df %>%
  select("title", "tag_name") %>%
  head(15) %>%
  rmarkdown::paged_table()
```

From the resulting data frame, we can see that `tag_name` does indeed describe
the genre of books it is associated with.

In order to answer our question, we need to collate data on how users review books from certain genres. 
To do so, let's build another pipeline to join with `ratings_df` and link users
with the genres of the books they rated. We can do a summary on the data frame
from there to end up with a table of mean ratings in each genre of interest
for all users. These ratings range from 1 to 5, with a mean rating of -5 to mark
genres with no rating.

```{r}
# Note: This takes awhile to run...

user_features_raw_df <- ratings_df %>%
  inner_join(book_genres_df, by="book_id") %>%
  spread(tag_name, rating) %>%
  select(-book_id, -title,)

user_features_df <- user_features_raw_df %>%
  group_by(user_id) %>%
  summarize_all(mean, na.rm=TRUE) %>%
  select(-user_id,) %>%
  mutate_if(~ any(is.na(.x)),~ if_else(is.na(.x),-5,.x)) # Replace na with -5

# Convert column names to snake_case
colnames(user_features_df) = to_snake_case(colnames(user_features_df))

head(user_features_df, 15) %>%
  mutate_all(round, 2) %>%
  rmarkdown::paged_table()
```

## Visualization and Analysis

Now that we have transformed the original data into a data frame of average
genre ratings for all users, we can begin analyzing users
based on genre ratings.

One way to better understand data in general is through visualization.
For example, if we want to see where the readership lies on
ratings for young adult versus fantasy, we can do so by plotting the average ratings
from both genres on a 2D scatterplot.

```{r, fig.align='center'}
ggplot(user_features_df, mapping=aes(x=fantasy, y=young_adult)) + 
# If you don't want to see user averages for users who have not rated books from either genre, uncomment the next two lines
#  scale_x_continuous(limits = c(0, 5)) + 
#  scale_y_continuous(limits = c(0, 5)) + 
  geom_point() +
  labs(
    title="User Average Ratings for Young Adult Versus Fantasy",
    x="Fantasy Rating",
    y="Young Adult Ratings"
  )
```

From the
graph, it appears most user have average ratings clustered between 3 and 5 with no
strong preference for one genre versus the other. There are
also several users that appear to be readers of one
genre but not the other, and at least one user that
does not read from either.

### Clustering for Higher Dimensions

While a 2D graph is fine for comparing two genres, it becomes harder
to visualize data like this beyond 3 variables since we run out of spatial dimensions.
Furthermore, we are also interested in clustering our data. While we had done a very cursory
grouping by eye-balling the data above, ideally, there should be a way to more formally
say which data points form a group with one another.

The solution to both of these problems is found in two methods known as [**K-means clustering**](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)
and [**principal component analysis (PCA)**](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c).

K-means clustering is a way to cluster, or group, data points. The algorithm designates
K centroids representing the centers of their respective clusters. Each data point is assigned
to the closest centroid, and the algorithm iteratively optimizes the centroids to try to reduce the total distance
between it and all the points in its cluster.

![K-means on iris flower species](images/kmeans_iris.png)

Meanwhile, principal component analysis (PCA) is a way to reduce the dimensions in high-dimensional data while
still trying to preserve features from the original space. Roughly, PCA tries to find a set of linear
transformations from the original data that prioritizes maximizing variance for each dimension of the transformed data.
At the end, the first two dimensions of the transformed data will ideally encode enough of the variance from the original
high-dimensional data that graphing only those two dimensions should suffice in representing the original data, making PCA a very useful tool for visualization.

![Example PCA: The transformed coordinate system in red maximizes the variance of the pixel locations.](images/pcafish.png)

When used in conjunction, K-means clustering will allow us to formally cluster
multi-dimensional data and PCA will allow us graph the clustered data in 2D for easy
visualization.

### Determining K

A key roadblock with using K-means is that we do not have a good idea of what is a "good" K, or number of clusters. One way we can
address this is by doing several test runs with different K's and comparing them, specifically,
by comparing their total **sum of squared distances (SSE)**. Intuitively, a good clustering should
minimize the distances of points to the centroids of their respective clusters which would be reflected
in a low total SSE [@kmeansTDS].

While we want to lower variation within clusters, at the same time, we also want our clusters to actually be
clusters. This means that for any data point, it should ideally be close to the centroid of its cluster
and far from others, a metric known as the **silhouette** [@kmeansUCR]. While increasing K to an
unreasonably large number may reduce total SSE, data in such a clustering would have low silhouettes since data points in one
cluster would be very close to data points in other clusters.

The goal of finding a good K then resides in reducing SSE while still trying to keep silhouette high. Let's
compute and graph both SSE and silhouette from a sample of the users to see what a good K might be.

```{r, fig.align='center'}
set.seed(320)

k_range <- 2:20
user_features_sample_df <- sample_n(user_features_df, 1000)

# SSE
sse <- sapply(k_range, function(k) {
  kmeans(user_features_sample_df, k)$tot.withinss
})

ggplot(mapping=aes(x=k_range, y=sse)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title="Total SSE vs K",
    x="k",
    y="Total SSE"
  )

# Average silhouettes
sil <- sapply(k_range, function(k) {
  km.res <- kmeans(user_features_sample_df, centers=k)
  ss <- silhouette(km.res$cluster, dist(user_features_sample_df))
  mean(ss[,3])
})

ggplot(mapping=aes(x=k_range, y=sil)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title="Average Silhouettes vs K",
    x="k",
    y="Average Silhouette"
  )
```

Observing both graphs, K = 6 appears to be a point where total SSE has begun to flatten but
average silhouette is still somewhat high, making it a reasonable compromise for K. Above K = 6, the total SSE does decrease,
but not significantly enough to warrant a corresponding decrease in average silhouette. With K
in hand, we can run K-means clustering on the full data now and graph using PCA.

```{r, fig.align='center'}
set.seed(320)

kmeans_res <- kmeans(user_features_df, 6)
kmeans_res$centers %>%
  as_tibble() %>%
  mutate_all(round, 2) %>%
  rmarkdown::paged_table()

# PCA
pca_res <- prcomp(user_features_df, center=TRUE, scale.=TRUE)
plot_data <- pca_res$x[,1:2] %>%
  as_tibble() %>%
  mutate(cluster=factor(kmeans_res$cluster))
 
ggplot(plot_data, aes(PC1, PC2, color=cluster)) +
  geom_point() + 
  labs(title="Readership Clusters")
```

## Making Predictions

Here in this section we talk about prediction analysis using R, specifically with regards to basic machine learning. In the following code chunk, we take our previous results from k-means clustering, and then append them to our previous dataframe in order to augment it into a **training data frame**. Notice also here that we're also generating the individual Principal Components used in PCA before. The intent here is that we can select a subset of the most important (cover the most variability) Principal Components and use those in other prediction methods. We've also included a graph of the PC's and how they correspond to clustering with the k-means algorithm.	


```{r}
set.seed(1234)	
# Set up training data for cluster prediction	
training_data_ratings <- user_features_df %>%	
  mutate(cluster=factor(kmeans_res$cluster)) %>%	
  sample_n(5000) #otherwise we have way too much data, can also try < 10k	
# Setting up training data for PCA anlaysis	
training_data_pca <-  as.data.frame(predict(pca_res, training_data_ratings)) %>% as_tibble()	
training_data_pca$cluster <- training_data_ratings$cluster	
training_data_ratings %>%	
  head(15) %>%	
  rmarkdown::paged_table()	
training_data_pca %>%	
  head(15) %>%	
  rmarkdown::paged_table()
```

### Random Forests	

Now that we've generated our training data set, we can begin training our own machine learning (ML) model. First, we'll take a look at a very popular tree-based model known as [**random forest**](https://towardsdatascience.com/understanding-random-forest-58381e0602d2). In a regular decision tree, the idea is to break the classification problem of ML up into multiple partitions first, and this forms the basis of the actual tree. With random forests, we take this a step further by creating multiple, random decision trees and then averaging the results from each of them for the classification problem. This improves prediction and also reduces instability from just singular decision trees.

```{r}	
# Training classification on dataframe using randomForest (n = 200):	
rf_prediction <- randomForest(cluster ~ .,	
                              ntree=200,	
                              importance=TRUE,	
                              data=training_data_ratings)	
```	

Now that we've generated our random forest model using all of our elements/predictors (the ratings of each genre) and sampling 200 decision trees, we can now take a look at the actual importance of each predictor. The importance of each predictor is judged by the **Mean Decrease in Accuracy**, which is calculated as the decrease in accuracy when the predictor is removed from the model, allowing us to see which predictors are the most influential. When comparing mean decrease in accuracy, a larger mean decrease for one predictor over another means that if the first predictor were removed, it would result in the model making on average more misclassifications than the second.

```{r, fig.align='center'}
variable_importance <- importance(rf_prediction)

rf_predictors_importance <- variable_importance %>%
  as.data.frame() %>%
  rownames_to_column(var="predictors")

rf_predictors_importance %>%
  ggplot(aes(x=reorder(predictors, MeanDecreaseAccuracy), y=MeanDecreaseAccuracy)) +
  geom_bar(stat="identity") +
  coord_flip() +
  labs(title="Genres by Importance", y="Mean Decrease Accuracy", x="Genre")
```

From the graph, it seems that ratings in certain genres such as realistic fiction and horror
are able to better classify users.

### Support Vector Machines

As an additional example, we'll also show how to train using a **support vector machine** (SVM) multi-classification model
as an additional factor in our experiment below. The way SVM works, is to actually **increase dimension** as compared to PCA.
Basically, we want to create a hyperplane - some n-dimensional vector that can effectively separate between multiple classes
in the classification problem. As an example of how this work, consider a simple 2 group classification problem in 2D:

![Example SVM: Moving from 2D coordinate space to 3D in order to better separate classification](images/svm_example.png)

In the example above, the 2D plot is circular, meaning there is no easy, linear division between the red and blue points.
However, if we extend to 3D (by letting $z = x^2 + y^2$) we get a shape in 3D that we can divide using a plane. Notice
that by translating this plane into 2D, we actually get a circle although in 3D it is "linear/planar". The "support vectors"
in SVM refer to the data points closest to the hyperplane, which are used to help determine the hyperplane itself. In the next
example, we use the ratings training data set to train our SVM model:

```{r}
set.seed(1234)    # Setting seed for reproducibility

n <- nrow(training_data_ratings)  # Number of observations
ntrain <- round(n*0.75)  # 75% for training set
split_index <- sample(n, ntrain)   # Create a random index
train_svm <- training_data_ratings[split_index,]   # Create training set
test_svm <- training_data_ratings[-split_index,]   # Create test set

svm_prediction <- svm(cluster ~ .,
                      data=train_svm, 
                      method="C-classification",
                      kernal="radial", 
                      gamma=0.1,
                      cost=10) #coefficients determined w/ help from guide
```

Now we can visualize our hyperplane and the way our SVM model is classifying using an in-built function for SVM. For
this, we'll consider the differences between ratings for realistic fiction and horror (2 predictors of importance according
to our previous random forest model) while keeping the others constant:

**TODO: this graph needs to be made more clear**

```{r, fig.align='center'}
# using plot() to visualize our hyperplane:
plot(svm_prediction, train_svm, realistic_fiction ~ horror,
          slice=list(fantasy = 3, mystery = 3, young_adult = 3, romance = 3, science_fiction = 3, thriller = 3))
```

Next we can actually measure our accuracy for the SVM model by using it to predict results for our testing data set
and plotting a confusion matrix:

**TODO: can talk about confusion matrix too**

```{r}
pred_svm <- predict(svm_prediction, test_svm)

table(test_svm$cluster, pred_svm)
prop.table(table(test_svm$cluster, pred_svm), 1)
```

### Evaluating Model Performance

From the above, we see that random forest determined/calculated the importance of each of the genre ratings as predictors for the
cluster group. From the barplot, it seems that fantasy, mystery and young_adult were one of the most important predictors. Now,
let's analyze the accuracy of our randomForest model by generating ROC curves, which can give us a measure of just how accurate/how
well our model fits the dataset. For the next section, we'll be analyzing our random forest model on the raw training data set,
and comparing that to our PCA-created data set and see the differences in performance.  

```{r}
set.seed(1234)

# setup training parameters
fit_control <- trainControl( ## 5-fold CV
  method = "cv",
  number = 5)

evaluate_model <- function(training_data) {
  # We use 5-fold Cross Validation as our experiment to determine the accuracy of our random_forest model:
  # create the cross-validation partition
  cv_partition <- createFolds(training_data$cluster, k=5)

  # iterate over folds for normal random forest data:
  rf_res <- sapply(seq_along(cv_partition),  function(i) {
    # train the random forest using train() instead of randomForest, 10 trees
    fit <- train(cluster ~ .,
                 data = training_data[-cv_partition[[i]],], # all but the holdout set
                 method = "rf",
                 ntree = 10,
                 trControl = fit_control)
    
    # make predictions on the holdout set
    preds <- predict(fit, training_data[cv_partition[[i]],])
      
    sum(preds == training_data$cluster[cv_partition[[i]]]) / length(preds) # output percentage correctly identified for now
    
    # compute tpr and fpr from the hold out set
    #perf <- ROCR::prediction(preds, training_data$cluster[cv_partition[[i]]]) %>%
    #  ROCR::performance(measure="tpr", x.measure="fpr")
  
    #fpr <- unlist(perf@x.values)
    #tpr <- unlist(perf@y.values)
      
    # interpolate the roc curve over 0, 1 range
    #interp_tpr <- approxfun(fpr, tpr)(mean_fpr)
    #interp_tpr[1] <- 0.0
      
    # collect values for this fold
    #data_frame(fold=rep(i, length(mean_fpr)), fpr=mean_fpr, tpr=interp_tpr)
  })
  
  # combine values across all folds for normal random forest data
  # into a single data frame
  #rf_normal_curve_df <- do.call(rbind, rf_res_normal)
  
  # get AUC results:
  #rf_normal_auc_df <- rf_normal_curve_df %>% group_by(fold) %>%
  #  summarize(auc=pracma::trapz(fpr, tpr))
  
  rf_res
}

rf_ratings_res <- evaluate_model(training_data_ratings)
rf_pca_res <- evaluate_model(training_data_pca)
```

Now let's perform the exact same procedue on our PCA trained data set instead:

Now that we've conducted our experiment, we can talk about the specific methods of model evaluation including: Area Under Curve (AUC) and ROC which we've graphed below:

**TODO: Write up + Explain more about AUC/ROC**

```{r, eval = FALSE}
# combine performance data for both models
# into one data frame (adding column to indicate)
# which model was used
curve_df <- rf_normal_curve_df %>%
  mutate(model="normal") %>%
  rbind(mutate(rf_pca_curve_df, model="PCA")) %>%
  mutate(model = factor(model, levels=c("normal", "PCA")))

auc_df <- rf_normal_auc_df %>%
  mutate(model="normal") %>%
  rbind(mutate(rf_pca_auc_df, model="PCA")) %>%
  mutate(model = factor(model, levels=c("normal", "PCA")))

# plot distribution of 
ggplot(auc_df, aes(x=model, y=auc)) +
  geom_jitter(position=position_jitter(0.1)) +
  coord_flip() + 
  labs(title="AUC comparision",
       x="Model",
       y="Area under ROC curve")

```

We can also test for differences using linear regression:

```{r, eval = FALSE}
model_tab <- auc_df %>%
  lm(auc~model,data=.) %>%
  tidy() 

model_tab %>%
  knitr::kable()
```

Now let's compare ROC curves for both the regular training data and the PCA trained data set:

```{r, eval = FALSE}
curve_df %>%
  group_by(model, fpr) %>%
  summarize(tpr = mean(tpr)) %>%
  ggplot(aes(x=fpr, y=tpr, color=model)) +
    geom_line() +
    labs(title = "ROC curves",
         x = "False positive rate",
         y = "True positive rate")

```

**TODO: Write up results for ROC Curves graph**


## Results and Discussion

Let's try to qualitatively summarize our clusters:

  1. Rates average 4 in all genres but doesn't read realistic fiction
  2. Rates average 4 in all genres
  3. Rates average 3-4 in all genres but dislikes SF and doesn't read horror
  4. Rates average 4.5-5 in all genres
  5. Rates average 3.5 in all genres 
  6. Rates average 3 all genres

Observing the graph of clusters, groups 2, 4, 5, and 6 appear to be located
close to each other. Among the clusters, groups 1, 3, and 6 show the
largest amounts of variation within their clusters while groups
2, 4, and 5 are tightly clustered.

The close locations of groups 2, 4, 5, and 6 mean that the user
ratings in these groups are harder to distinguish from each other which
makes sense as these are all groups that are similar to each other in that they have
users who have constant average ratings across all genres.

Observing group 6, both its large variation
and slight intersection into group 5 seem to indicate that while the 
average ratings are around 3, there are probably a substantial amount of users
within the group that rate very low, dragging the averages down.

Group 3 is also an interesting case. The cluster has large variation and a
somewhat specific qualitative description. Moreover, informal observation of the
cluster graph seems to show smaller clusters within this group as well. This seems to indicate
that while the users in this group generally have the qualitative characteristics as
described above in common, there are still sub-groups within this cluster
that have varying defining characteristics from each other.

Group 1 is another cluster with a specific qualitative description. The variation
within this cluster is somewhat high and the cluster is located far away from
other the other groups. This seems to indicate that there truly is a common group
of readers that do not read realistic fiction, but they also likely vary in their average
ratings across other genres.

**TODO: Add more on predictions**

Looking at our predictions, we got some very interesting results. By training our Random Forest model, we were able to show on the regular dataset that interestingly, the most "important" predictors for classification seemed to be fantasy, young adult, and science fiction
genre ratings.

**TODO: Write up SVM**

**TODO: Write up ROC, AUROC**

While we were able to successfully cluster users in this project, there
are still several potential areas for improvement. One issue was that by only
working with the average genre ratings for each user, we lost
information about the variance of ratings within each genre per user. This means that
certain users may have ended up in the same cluster because they had similar average ratings
but still rate books differently in reality because the variation in how they rate was not accounted for.

Another area that could have been improved was our clustering algorithm.
K-means clustering was not able to find some of the smaller clusters in group 6,
possibly due to a large amount of users in other locations which ended up occupying the algorithm.
Since there were more data points densely packed in the area near groups 2 and 5, the centroids
for K-means ended up gathering near there and ignoring smaller clusters elsewhere.

**TODO: Talk more on improvements**

## Conclusion

In this project, we demonstrated how data from Goodreads could be used to find
common groups of users among the site's readership. We were able to do this through
a combination of data manipulation with pipelines and statistical analyses with K-means
clustering and PCA.

These clusterings revealed some insights, such as the existence of a large group of users
that do not read realistic fiction and a large group of users that have little interest in SF and
horror. While our clustering was successful, there are still several areas of improvement we could
have addressed for characterizing users, such as accounting for rating variation
per user and the use of more advanced clustering algorithms.

Clustering a site's userbase is a problem that extends beyond Goodreads. In general, being able to find
natural clusterings of users is both useful and powerful in that it allows for a large number of people
to be bucketed into human-understandable categories. Knowing these clusters can let platforms
gain a better sense of their userbase and allow them to better target content and appropriately address their communities.

## References

<div class="pixel"></div>
<div class="holo"></div>
