---
title: "Analyzing Goodreads' Readership"
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
library("tidyverse")
library("snakecase")
library("ggfortify")
library("cluster")
```

[Goodreads](https://www.goodreads.com) is a popular social cataloging website where users can search, track, and rate books. The site has
over 20 million users and 50 million book reviews, making it a very large source of readership data.

![Goodreads site](./images/screenshot_01.png)


With so many users and reviews, an interesting experiment would be to see if
we can generalize the readership of Goodreads based on how users rate books from
certain genres. From this, we would be able to see if, for instance, there is
a large group of sci-fi fans or a large group of romance and YA fanatics that
also hate horror.

In this project, we demonstrate the process of doing this kind of analysis from the ground up by leveraging and manipulating
a subset of Goodreads' data using the Goodbooks-10k dataset [@goodbooks2017] and performing
statistical and cluster analysis to try to try to paint a generalized picture of the site's readership.

## Loading and Pre-processing Data

Goodbooks-10k consists of several CSV files containing information about the top 10,000 most rated
books on Goodreads, including book metadata, tags, and relevant user ratings.

The CSV files can be easily loaded using R's `read.csv` method. For example, here we load and display
`books.csv` to get a broad picture of one of the datasets we will be working
with. We can see that `books.csv` organizes its data in a tabulated manner with each book as a row
and each book attribute as a column.

```{r}
books_df <- read.csv("./goodbooks-10k/books.csv") %>%
  as_tibble()
head(books_df)
```

Let's load the rest of our data as well.

```{r}
ratings_df <- read.csv("./goodbooks-10k/ratings.csv")%>%
  as_tibble()
tags_df <- read.csv("./goodbooks-10k/tags.csv")%>%
  as_tibble()
book_tags_df <- read.csv("./goodbooks-10k/book_tags.csv")%>%
  as_tibble()
```

With our data loaded, we can begin to do some data manipulation. Since we are interested
in analyzing various genres, let's start by narrowing down `tags_df` with
the genre tags we are interested in. We can do this using a **pipeline**
as shown below.

```{r}
tags_df <- tags_df %>%
  filter(tag_name %in% c(
    "fantasy",
    "mystery",
    "horror",
    "realistic-fiction",
    "romance",
    "science-fiction",
    "thriller",
    "young-adult"
  )) %>%
  as_tibble()

tags_df
```

However, this tag information is not very helpful by itself. We
want to be able to use it in conjunction with the other data
we have. One way to accomplish this is by **joining** data across
different data frames through a common attribute. Here, we construct a pipeline
to link data from `books_df` and `book_tags_df` through the shared
attribute, `goodreads_book_id`. From there, we then link `book_tags_df` and `tags_df`
through `tag_id`. From the resulting data frame, we can see that `tag_name` does indeed describe
the genre of books it is associated with.

```{r}
book_genres_df <- books_df %>%
  inner_join(book_tags_df, by="goodreads_book_id") %>%
  inner_join(tags_df, by="tag_id") %>%
  select("book_id", "title", "tag_name")

book_genres_df %>%
  select("title", "tag_name") %>%
  head(15)
```

Let's build another pipeline to join with `ratings_df` and link users
with the genres of the books they rated. We can do a summary on the data frame
from there to end up with a table of mean ratings in each genre of interest
for all users. These ratings range from 1 to 5, with a mean rating of -5 to mark
genres with no rating.

```{r}
# TODO: What should we replace na with??

# Note: This takes awhile to run...

user_features_raw_df <- ratings_df %>%
  inner_join(book_genres_df, by="book_id") %>%
  spread(tag_name, rating) %>%
  select(-book_id, -title,)

user_features_df <- user_features_raw_df %>%
  group_by(user_id) %>%
  summarize_all(mean, na.rm=TRUE) %>%
  select(-user_id,) %>%
  mutate_if(~ any(is.na(.x)),~ if_else(is.na(.x),-5,.x)) # Replace na with -5

# Convert column names to snake_case
colnames(user_features_df) = to_snake_case(colnames(user_features_df))

head(user_features_df, 15)
```

## Data Visualization and Analysis

Now that we have transformed the original data into a data frame of average
genre ratings for all users, we can begin analyzing users
based on genre ratings.

One way to better understand data in general is through visualization.
For example, if we want to see where the readership lies on
ratings for young adult versus fantasy, we can do so by plotting the average ratings
from both genres on a 2D scatterplot.

```{r, fig.align='center'}
ggplot(user_features_df, mapping=aes(x=fantasy, y=young_adult)) +
  geom_point() +
  labs(
    title="User Average Ratings for Young Adult Versus Fantasy",
    x="Fantasy Rating",
    y="Young Adult Ratings"
  )
```

From the
graph, it appears most user have average ratings clustered between 3 and 5 with no
strong preference for one genre versus the other. There are
also several users that appear to be readers of one
genre but not the other, and at least one user that
does not read from either.

### Clustering for Higher Dimensions

While a 2D graph is fine for comparing two genres, it becomes harder
to visualize data like this beyond 3 variables since we run out of spatial dimensions.
Furthermore, we are also interested in clustering our data. While we had done a very cursory
grouping by eye-balling the data above, ideally, there should be a way to more formally
say which data points form a group with one another.

The solution to both of these problems is found in two methods known as **K-means clustering**
and **principal component analysis (PCA)**.

K-means clustering is a way to cluster, or group, data points. The algorithm designates
K centroids representing the centers of their respective clusters. Each data point is assigned
to the closest centroid, and the algorithm iteratively optimizes the centroids try to reduce the total distance
between it and all the points in its cluster.

![K-means on iris flower species](images/kmeans_iris.png)

Meanwhile, principal component analysis (PCA) is a way to reduce the dimensions in high-dimensional data while
still trying to preserve features from the original space. Roughly, PCA tries to find a set of linear
transformations from the original data that prioritizes maximizing variance for each dimension of the transformed data.
At the end, the first two dimensions of the transformed data will ideally encode enough of the variance from the original
high-dimensional data that graphing only those two dimensions should suffice in representing the original data, making PCA a very
a useful tool for visualization.

![Example PCA: The transformed coordinate system in red maximizes the variance of the pixel locations.](images/pcafish.png)

When used in conjunction, K-means clustering will allow us to formally cluster
multi-dimensional data and PCA will allow us graph the clustered data in 2D for easy
visualization.

### Determining K

A key roadblock with using K-means is that we do not have a good idea of what is a "good" K, or number of clusters. One way we can
address this is by doing several test runs with different K's and comparing them, specifically,
by comparing their total **sum of squared distances (SSE)**. Intuitively, a good clustering should
minimize the distances of points to the centroids of their respective clusters which would be reflected
in a low total SSE [@kmeansTDS].

While we want to lower variation within clusters, at the same time, we also want our clusters to actually be
clusters. This means that for any data point, it should ideally be close to the centroid of its cluster
and far from others, a metric known as the **silhouette** [@kmeansUCR]. While increasing K to an
unreasonably large number may reduce total SSE, data in such a clustering would have low silhouettes since data points in one
cluster would be very close to data points in other clusters.

The goal of finding a good K then resides in reducing SSE while still trying to keep silhouette high. Let's
compute and graph both SSE and silhouette from a sample of the users to see what a good K might be.

```{r, fig.align='center'}
set.seed(320)

k_range <- 2:20
user_features_sample_df <- sample_n(user_features_df, 1000)

# SSE
sse <- sapply(k_range, function(k) {
  kmeans(user_features_sample_df, k)$tot.withinss
})

ggplot(mapping=aes(x=k_range, y=sse)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title="Total SSE vs K",
    x="k",
    y="Total SSE"
  )

# Average silhouettes
sil <- sapply(k_range, function(k) {
  km.res <- kmeans(user_features_sample_df, centers=k)
  ss <- silhouette(km.res$cluster, dist(user_features_sample_df))
  mean(ss[,3])
})

ggplot(mapping=aes(x=k_range, y=sil)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title="Average Silhouettes vs K",
    x="k",
    y="Average Silhouette"
  )
```

Observing both graphs, K = 6 appears to be a point where total SSE has begun to flatten but
average silhouette is still somewhat high, making it a reasonable compromise for K. With K
in hand, we can run K-means clustering on the full data now.

```{r, fig.align='center'}
set.seed(320)

kmeans_res <- kmeans(user_features_df, 6)
kmeans_res$centers

# Autoplot automatically does PCA
autoplot(kmeans_res, data=user_features_df, frame=TRUE) +
  labs(title="Readership Clusters")
```

**TODO: talk about clusters**

## Predictions

```{r}
# TODO: cluster prediction (logistic regression? decision tree?)

training_data <- user_features_df %>%
  mutate(cluster=kmeans_res$cluster)

head(training_data, 15)
```

## Conclusion

**TODO: Write conclusion**

## References
