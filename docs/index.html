<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Analyzing Goodreads’ Readership</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/paper.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 64px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h2 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h3 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h4 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h5 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h6 {
  padding-top: 69px;
  margin-top: -69px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="/">← Back to Home</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Analyzing Goodreads’ Readership</h1>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><a href="https://www.goodreads.com">Goodreads</a> is a popular social cataloging website where users can search, track, and rate books. The site also offers a variety of recommendation services that provide users a way to find books relevant to their reading history. Since its foundation in 2006, the site’s userbase has grown to 20 million users with over 50 million book reviews, making it a very large <a href="https://github.com/zygmuntz/goodbooks-10k">source of readership data</a>.</p>
<div class="figure">
<img src="images/screenshot_01.png" alt="Goodreads site" />
<p class="caption">Goodreads site</p>
</div>
<p>With so many users and reviews, an interesting experiment would be to see if we can characterize the readership of Goodreads based on user ratings. Specifically we are interested in answering the question, “Based on user ratings data, can we find common groups of users by how they rate certain genres of books?” With such information, we would be able to see if, for instance, there is a large group of sci-fi fans or a large group of romance and young adult fanatics that also happen to hate horror.</p>
<blockquote>
<p>“Based on user ratings data, can we find common groups of users by how they rate certain genres of books?”</p>
</blockquote>
<p>In this project, we demonstrate the process of doing this kind of analysis from the ground up by leveraging and manipulating a subset of Goodreads’ data using the <a href="https://github.com/zygmuntz/goodbooks-10k">Goodbooks-10k dataset</a> <span class="citation">(Zajac 2017)</span> and performing statistical and <a href="https://www.statisticssolutions.com/directory-of-statistical-analyses-cluster-analysis/">cluster analysis</a> to try to try to paint a generalized picture of the site’s readership.</p>
</div>
<div id="understanding-and-pre-processing-the-data" class="section level2">
<h2>Understanding and Pre-processing the Data</h2>
<p>The Goodbooks-10k data obtained from <a href="https://github.com/zygmuntz/goodbooks-10k">GitHub</a> consists of several CSV files with information about the top 10,000 most rated books on Goodreads, including book metadata, tags, and relevant user ratings.</p>
<p>The CSV files can be easily loaded using R’s <code>read.csv</code> method. For example, here we load and display <code>books.csv</code> to get a broad picture of one of the datasets we will be working with. We can see that <code>books.csv</code> organizes its data in a tabulated manner with each book as a row and each book attribute as a column.</p>
<pre class="r"><code>books_df &lt;- read.csv(&quot;./goodbooks-10k/books.csv&quot;) %&gt;%
  as_tibble()
head(books_df) %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["book_id"],"name":[1],"type":["int"],"align":["right"]},{"label":["goodreads_book_id"],"name":[2],"type":["int"],"align":["right"]},{"label":["best_book_id"],"name":[3],"type":["int"],"align":["right"]},{"label":["work_id"],"name":[4],"type":["int"],"align":["right"]},{"label":["books_count"],"name":[5],"type":["int"],"align":["right"]},{"label":["isbn"],"name":[6],"type":["fctr"],"align":["left"]},{"label":["isbn13"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["authors"],"name":[8],"type":["fctr"],"align":["left"]},{"label":["original_publication_year"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["original_title"],"name":[10],"type":["fctr"],"align":["left"]},{"label":["title"],"name":[11],"type":["fctr"],"align":["left"]},{"label":["language_code"],"name":[12],"type":["fctr"],"align":["left"]},{"label":["average_rating"],"name":[13],"type":["dbl"],"align":["right"]},{"label":["ratings_count"],"name":[14],"type":["int"],"align":["right"]},{"label":["work_ratings_count"],"name":[15],"type":["int"],"align":["right"]},{"label":["work_text_reviews_count"],"name":[16],"type":["int"],"align":["right"]},{"label":["ratings_1"],"name":[17],"type":["int"],"align":["right"]},{"label":["ratings_2"],"name":[18],"type":["int"],"align":["right"]},{"label":["ratings_3"],"name":[19],"type":["int"],"align":["right"]},{"label":["ratings_4"],"name":[20],"type":["int"],"align":["right"]},{"label":["ratings_5"],"name":[21],"type":["int"],"align":["right"]},{"label":["image_url"],"name":[22],"type":["fctr"],"align":["left"]},{"label":["small_image_url"],"name":[23],"type":["fctr"],"align":["left"]}],"data":[{"1":"1","2":"2767052","3":"2767052","4":"2792775","5":"272","6":"439023483","7":"9.780439e+12","8":"Suzanne Collins","9":"2008","10":"The Hunger Games","11":"The Hunger Games (The Hunger Games, #1)","12":"eng","13":"4.34","14":"4780653","15":"4942365","16":"155254","17":"66715","18":"127936","19":"560092","20":"1481305","21":"2706317","22":"https://images.gr-assets.com/books/1447303603m/2767052.jpg","23":"https://images.gr-assets.com/books/1447303603s/2767052.jpg"},{"1":"2","2":"3","3":"3","4":"4640799","5":"491","6":"439554934","7":"9.780440e+12","8":"J.K. Rowling, Mary GrandPrÃ©","9":"1997","10":"Harry Potter and the Philosopher's Stone","11":"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)","12":"eng","13":"4.44","14":"4602479","15":"4800065","16":"75867","17":"75504","18":"101676","19":"455024","20":"1156318","21":"3011543","22":"https://images.gr-assets.com/books/1474154022m/3.jpg","23":"https://images.gr-assets.com/books/1474154022s/3.jpg"},{"1":"3","2":"41865","3":"41865","4":"3212258","5":"226","6":"316015849","7":"9.780316e+12","8":"Stephenie Meyer","9":"2005","10":"Twilight","11":"Twilight (Twilight, #1)","12":"en-US","13":"3.57","14":"3866839","15":"3916824","16":"95009","17":"456191","18":"436802","19":"793319","20":"875073","21":"1355439","22":"https://images.gr-assets.com/books/1361039443m/41865.jpg","23":"https://images.gr-assets.com/books/1361039443s/41865.jpg"},{"1":"4","2":"2657","3":"2657","4":"3275794","5":"487","6":"61120081","7":"9.780061e+12","8":"Harper Lee","9":"1960","10":"To Kill a Mockingbird","11":"To Kill a Mockingbird","12":"eng","13":"4.25","14":"3198671","15":"3340896","16":"72586","17":"60427","18":"117415","19":"446835","20":"1001952","21":"1714267","22":"https://images.gr-assets.com/books/1361975680m/2657.jpg","23":"https://images.gr-assets.com/books/1361975680s/2657.jpg"},{"1":"5","2":"4671","3":"4671","4":"245494","5":"1356","6":"743273567","7":"9.780743e+12","8":"F. Scott Fitzgerald","9":"1925","10":"The Great Gatsby","11":"The Great Gatsby","12":"eng","13":"3.89","14":"2683664","15":"2773745","16":"51992","17":"86236","18":"197621","19":"606158","20":"936012","21":"947718","22":"https://images.gr-assets.com/books/1490528560m/4671.jpg","23":"https://images.gr-assets.com/books/1490528560s/4671.jpg"},{"1":"6","2":"11870085","3":"11870085","4":"16827462","5":"226","6":"525478817","7":"9.780525e+12","8":"John Green","9":"2012","10":"The Fault in Our Stars","11":"The Fault in Our Stars","12":"eng","13":"4.26","14":"2346404","15":"2478609","16":"140739","17":"47994","18":"92723","19":"327550","20":"698471","21":"1311871","22":"https://images.gr-assets.com/books/1360206420m/11870085.jpg","23":"https://images.gr-assets.com/books/1360206420s/11870085.jpg"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Let’s load the rest of our data as well.</p>
<pre class="r"><code>ratings_df &lt;- read.csv(&quot;./goodbooks-10k/ratings.csv&quot;)%&gt;%
  as_tibble()
tags_df &lt;- read.csv(&quot;./goodbooks-10k/tags.csv&quot;)%&gt;%
  as_tibble()
book_tags_df &lt;- read.csv(&quot;./goodbooks-10k/book_tags.csv&quot;)%&gt;%
  as_tibble()</code></pre>
<p>With our data loaded, we can begin to do some data manipulation. Since we are interested in analyzing various genres, let’s start by narrowing down the tags dataframe, <code>tags_df</code>, with the genre tags we are interested in. We can do this using a <a href="https://cfss.uchicago.edu/notes/pipes/"><strong>pipeline</strong></a> as shown below.</p>
<pre class="r"><code>tags_df &lt;- tags_df %&gt;%
  filter(tag_name %in% c(
    &quot;fantasy&quot;,
    &quot;mystery&quot;,
    &quot;horror&quot;,
    &quot;realistic-fiction&quot;,
    &quot;romance&quot;,
    &quot;science-fiction&quot;,
    &quot;thriller&quot;,
    &quot;young-adult&quot;
  )) %&gt;%
  as_tibble()

tags_df %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["tag_id"],"name":[1],"type":["int"],"align":["right"]},{"label":["tag_name"],"name":[2],"type":["fctr"],"align":["left"]}],"data":[{"1":"11305","2":"fantasy"},{"1":"14821","2":"horror"},{"1":"20939","2":"mystery"},{"1":"25438","2":"realistic-fiction"},{"1":"26138","2":"romance"},{"1":"26837","2":"science-fiction"},{"1":"30358","2":"thriller"},{"1":"33114","2":"young-adult"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>However, this tag information is not very helpful by itself. We want to be able to use it in conjunction with the other data we have. One way to accomplish this is by <a href="http://www.datasciencemadesimple.com/join-in-r-merge-in-r/"><strong>joining</strong></a> data across different data frames through a common attribute. We construct a pipeline to link data from <code>books_df</code> and <code>book_tags_df</code> through the shared attribute, <code>goodreads_book_id</code>. From there, we then link <code>book_tags_df</code> and <code>tags_df</code> through <code>tag_id</code>, creating a new dataframe with an entry for each book’s genre tag.</p>
<pre class="r"><code>book_genres_df &lt;- books_df %&gt;%
  inner_join(book_tags_df, by=&quot;goodreads_book_id&quot;) %&gt;%
  inner_join(tags_df, by=&quot;tag_id&quot;) %&gt;%
  select(&quot;book_id&quot;, &quot;title&quot;, &quot;tag_name&quot;)

book_genres_df %&gt;%
  select(&quot;title&quot;, &quot;tag_name&quot;) %&gt;%
  head(15) %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["title"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["tag_name"],"name":[2],"type":["fctr"],"align":["left"]}],"data":[{"1":"The Hunger Games (The Hunger Games, #1)","2":"young-adult"},{"1":"The Hunger Games (The Hunger Games, #1)","2":"fantasy"},{"1":"The Hunger Games (The Hunger Games, #1)","2":"science-fiction"},{"1":"The Hunger Games (The Hunger Games, #1)","2":"romance"},{"1":"The Hunger Games (The Hunger Games, #1)","2":"thriller"},{"1":"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)","2":"fantasy"},{"1":"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)","2":"young-adult"},{"1":"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)","2":"mystery"},{"1":"Twilight (Twilight, #1)","2":"young-adult"},{"1":"Twilight (Twilight, #1)","2":"fantasy"},{"1":"Twilight (Twilight, #1)","2":"horror"},{"1":"Twilight (Twilight, #1)","2":"science-fiction"},{"1":"To Kill a Mockingbird","2":"young-adult"},{"1":"To Kill a Mockingbird","2":"realistic-fiction"},{"1":"To Kill a Mockingbird","2":"mystery"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>From the resulting data frame, we can see that <code>tag_name</code> does indeed describe the genre of books it is associated with.</p>
<p>In order to answer our question, we need to collate data on how users review books from certain genres. To do so, let’s build another pipeline to join with <code>ratings_df</code> and link users with the genres of the books they rated. We can do a summary on the data frame from there to end up with a table of mean ratings in each genre of interest for all users. These ratings range from 1 to 5, with a mean rating of -5 to mark genres with no rating.</p>
<pre class="r"><code># Note: This takes awhile to run...

user_features_raw_df &lt;- ratings_df %&gt;%
  inner_join(book_genres_df, by=&quot;book_id&quot;) %&gt;%
  spread(tag_name, rating) %&gt;%
  select(-book_id, -title,)

user_features_df &lt;- user_features_raw_df %&gt;%
  group_by(user_id) %&gt;%
  summarize_all(mean, na.rm=TRUE) %&gt;%
  select(-user_id,) %&gt;%
  mutate_if(~ any(is.na(.x)),~ if_else(is.na(.x),-5,.x)) # Replace na with -5

# Convert column names to snake_case
colnames(user_features_df) = to_snake_case(colnames(user_features_df))

head(user_features_df, 15) %&gt;%
  mutate_all(round, 2) %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["fantasy"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["horror"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["mystery"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["realistic_fiction"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["romance"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["science_fiction"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["thriller"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["young_adult"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"3.55","2":"3.29","3":"3.40","4":"3.51","5":"3.45","6":"3.14","7":"3.40","8":"3.68"},{"1":"4.85","2":"5.00","3":"4.90","4":"4.38","5":"4.12","6":"4.75","7":"4.67","8":"4.76"},{"1":"1.45","2":"1.50","3":"1.24","4":"2.00","5":"1.76","6":"1.17","7":"1.13","8":"1.83"},{"1":"3.90","2":"3.59","3":"3.55","4":"3.65","5":"3.44","6":"3.54","7":"3.18","8":"4.08"},{"1":"4.11","2":"4.03","3":"4.03","4":"3.95","5":"3.93","6":"4.11","7":"4.00","8":"3.95"},{"1":"4.20","2":"4.25","3":"4.14","4":"4.29","5":"4.43","6":"4.17","7":"4.25","8":"4.42"},{"1":"3.79","2":"3.78","3":"3.76","4":"4.00","5":"3.51","6":"3.76","7":"3.60","8":"3.88"},{"1":"4.09","2":"3.95","3":"3.81","4":"3.25","5":"3.17","6":"4.42","7":"3.40","8":"3.50"},{"1":"3.66","2":"3.67","3":"3.25","4":"3.37","5":"3.52","6":"3.58","7":"3.03","8":"3.63"},{"1":"4.02","2":"3.88","3":"3.92","4":"3.86","5":"3.76","6":"3.89","7":"3.96","8":"4.07"},{"1":"3.63","2":"3.69","3":"3.42","4":"3.59","5":"3.44","6":"3.40","7":"3.69","8":"3.93"},{"1":"3.54","2":"3.38","3":"3.85","4":"3.86","5":"3.56","6":"3.81","7":"3.93","8":"3.65"},{"1":"3.64","2":"4.00","3":"4.33","4":"4.29","5":"4.11","6":"4.00","7":"4.40","8":"4.24"},{"1":"3.61","2":"3.60","3":"3.60","4":"3.08","5":"3.32","6":"3.66","7":"3.31","8":"3.34"},{"1":"4.26","2":"4.45","3":"4.00","4":"4.35","5":"4.31","6":"4.33","7":"4.08","8":"4.38"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="visualization-and-analysis" class="section level2">
<h2>Visualization and Analysis</h2>
<p>Now that we have transformed the original data into a data frame of average genre ratings for all users, we can begin analyzing users based on genre ratings.</p>
<p>One way to better understand data in general is through visualization. For example, if we want to see where the readership lies on ratings for young adult versus fantasy, we can do so by plotting the average ratings from both genres on a 2D scatterplot.</p>
<pre class="r"><code>ggplot(user_features_df, mapping=aes(x=fantasy, y=young_adult)) + 
  geom_point() +
  labs(
    title=&quot;User Average Ratings for Young Adult Versus Fantasy&quot;,
    x=&quot;Fantasy Rating&quot;,
    y=&quot;Young Adult Ratings&quot;
  )</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>From the graph, it appears most user have average ratings clustered between 3 and 5 with no strong preference for one genre versus the other. There are also several users that appear to be readers of one genre but not the other, and at least one user that does not read from either.</p>
<div id="clustering-for-higher-dimensions" class="section level3">
<h3>Clustering for Higher Dimensions</h3>
<p>While a 2D graph is fine for comparing two genres, it becomes harder to visualize data like this beyond 3 variables since we run out of spatial dimensions. Furthermore, we are also interested in clustering our data. While we had done a very cursory grouping by eye-balling the data above, ideally, there should be a way to more formally say which data points form a group with one another.</p>
<p>The solution to both of these problems is found in two methods known as <a href="https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1"><strong>K-means clustering</strong></a> and <a href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"><strong>principal component analysis (PCA)</strong></a>.</p>
<p>K-means clustering is a way to cluster, or group, data points. The algorithm designates K centroids representing the centers of their respective clusters. Each data point is assigned to the closest centroid, and the algorithm iteratively optimizes the centroids to try to reduce the total distance between it and all the points in its cluster.</p>
<div class="figure">
<img src="images/kmeans_iris.png" alt="K-means on iris flower species" />
<p class="caption">K-means on iris flower species</p>
</div>
<p>Meanwhile, principal component analysis (PCA) is a way to reduce the dimensions in high-dimensional data while still trying to preserve features from the original space. Roughly, PCA tries to find a set of linear transformations from the original data that prioritizes maximizing variance for each dimension of the transformed data. At the end, the first two dimensions of the transformed data will ideally encode enough of the variance from the original high-dimensional data that graphing only those two dimensions should suffice in representing the original data, making PCA a very useful tool for visualization.</p>
<div class="figure">
<img src="images/pcafish.png" alt="Example PCA: The transformed coordinate system in red maximizes the variance of the pixel locations." />
<p class="caption">Example PCA: The transformed coordinate system in red maximizes the variance of the pixel locations.</p>
</div>
<p>When used in conjunction, K-means clustering will allow us to formally cluster multi-dimensional data and PCA will allow us graph the clustered data in 2D for easy visualization.</p>
</div>
<div id="determining-k" class="section level3">
<h3>Determining K</h3>
<p>A key roadblock with using K-means is that we do not have a good idea of what is a “good” K, or number of clusters. One way we can address this is by doing several test runs with different K’s and comparing them, specifically, by comparing their total <strong>sum of squared distances (SSE)</strong>. Intuitively, a good clustering should minimize the distances of points to the centroids of their respective clusters which would be reflected in a low total SSE <span class="citation">(Dabbura 2018)</span>.</p>
<p>While we want to lower variation within clusters, at the same time, we also want our clusters to actually be clusters. This means that for any data point, it should ideally be close to the centroid of its cluster and far from others, a metric known as the <strong>silhouette</strong> <span class="citation">(Boehmke, n.d.)</span>. While increasing K to an unreasonably large number may reduce total SSE, data in such a clustering would have low silhouettes since data points in one cluster would be very close to data points in other clusters.</p>
<p>The goal of finding a good K then resides in reducing SSE while still trying to keep silhouette high. Let’s compute and graph both SSE and silhouette from a sample of the users to see what a good K might be.</p>
<pre class="r"><code>set.seed(320)

k_range &lt;- 2:20
user_features_sample_df &lt;- sample_n(user_features_df, 1000)

# SSE
sse &lt;- sapply(k_range, function(k) {
  kmeans(user_features_sample_df, k)$tot.withinss
})

ggplot(mapping=aes(x=k_range, y=sse)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title=&quot;Total SSE vs K&quot;,
    x=&quot;k&quot;,
    y=&quot;Total SSE&quot;
  )</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Average silhouettes
sil &lt;- sapply(k_range, function(k) {
  km.res &lt;- kmeans(user_features_sample_df, centers=k)
  ss &lt;- silhouette(km.res$cluster, dist(user_features_sample_df))
  mean(ss[,3])
})

ggplot(mapping=aes(x=k_range, y=sil)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title=&quot;Average Silhouettes vs K&quot;,
    x=&quot;k&quot;,
    y=&quot;Average Silhouette&quot;
  )</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>Observing both graphs, K = 6 appears to be a point where total SSE has begun to flatten but average silhouette is still somewhat high, making it a reasonable compromise for K. Above K = 6, the total SSE does decrease, but not significantly enough to warrant a corresponding decrease in average silhouette. With K in hand, we can run K-means clustering on the full data now and graph using PCA.</p>
<pre class="r"><code>set.seed(320)

kmeans_res &lt;- kmeans(user_features_df, 6)
kmeans_res$centers %&gt;%
  as_tibble() %&gt;%
  mutate_all(round, 2) %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["fantasy"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["horror"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["mystery"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["realistic_fiction"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["romance"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["science_fiction"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["thriller"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["young_adult"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"4.08","2":"4.06","3":"4.09","4":"-5.00","5":"4.03","6":"3.99","7":"4.00","8":"4.03"},{"1":"4.07","2":"3.95","3":"4.03","4":"3.96","5":"3.98","6":"4.04","7":"3.99","8":"4.07"},{"1":"3.54","2":"-5.00","3":"3.99","4":"3.88","5":"3.96","6":"1.90","7":"3.13","8":"4.09"},{"1":"4.57","2":"4.53","3":"4.56","4":"4.48","5":"4.52","6":"4.56","7":"4.55","8":"4.56"},{"1":"3.62","2":"3.45","3":"3.58","4":"3.54","5":"3.54","6":"3.57","7":"3.53","8":"3.63"},{"1":"3.11","2":"2.96","3":"3.11","4":"3.13","5":"3.10","6":"2.59","7":"2.69","8":"3.15"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>autoplot(kmeans_res, data=user_features_df, frame=TRUE) +
  labs(title=&quot;Readership Clusters&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We can try to qualitatively summarize our clusters:</p>
<ol style="list-style-type: decimal">
<li>Rates average 4 in all genres but doesn’t read realistic fiction</li>
<li>Rates average 4 in all genres</li>
<li>Rates average 3-4 in all genres but dislikes sci-fi and doesn’t read horror</li>
<li>Rates average 4.5-5 in all genres</li>
<li>Rates average 3.5 in all genres</li>
<li>Rates average 3 in all genres</li>
</ol>
</div>
</div>
<div id="making-predictions" class="section level2">
<h2>Making Predictions</h2>
<p>Now that we have done some analysis on our data and characterized Goodreads’ users into clusters, let’s see if we can predict which clusters users will land in using some basic machine learning (ML) techniques.</p>
<p>Before building a model, we need to split the data we will be working with. Data is split into two classes: one set for training the model and the other for testing the model. Let’s take a sample of our users and split them into training and testing datasets.</p>
<pre class="r"><code>set.seed(1234)

# Set up training data for cluster prediction   
user_sample &lt;- user_features_df %&gt;%
  mutate(cluster=factor(kmeans_res$cluster)) %&gt;%
  sample_n(5000)

# Split training-testing 3:1
n &lt;- nrow(user_sample)
ntrain &lt;- round(n*0.75)
split_index &lt;- sample(n, ntrain)
train_data_ratings &lt;- user_sample[split_index,]
test_data_ratings &lt;- user_sample[-split_index,]</code></pre>
<p>Now that we have split our data, we will test two types of classification models: random forests and state vector machines.</p>
<div id="random-forests" class="section level3">
<h3>Random Forests</h3>
<p>First, we’ll take a look at a popular tree-based model known as a <a href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2"><strong>random forest</strong></a>. Random forests are groups of decision trees used together to make classification decisions. For a given classification, every tree makes its own decision and the results from all trees are aggregated together at the end to determine the final classification. The benefit of using a random forest versus a single tree is that, generally, the random forest improves prediction results and reduces instability from using singular decision trees.</p>
<div class="figure">
<img src="images/random_forest.png" alt="Random forest example" />
<p class="caption">Random forest example</p>
</div>
<p>We can train a random forest with 200 trees using the code shown below.</p>
<pre class="r"><code>rf_model &lt;- randomForest(cluster ~ .,   
                              ntree=200,    
                              importance=TRUE,  
                              data=train_data_ratings)  </code></pre>
<p>Training a random forest model has the added benefit of allowing us to see which predictors are the most important for a classification decision. The importance of a predictor is determined using the <strong>mean decrease in accuracy</strong>. Mean decrease in accuracy of a predictor is the decrease in accuracy when the predictor is removed from the model and allows us to see which predictors are the most influential. When comparing mean decrease in accuracy, a larger mean decrease for one predictor over another means that if the first predictor were removed, it would result in the model making on average more misclassifications than if the second predictor were removed.</p>
<p>We extract and plot the mean decrease in accuracy for all genres in the following:</p>
<pre class="r"><code>variable_importance &lt;- importance(rf_model)

rf_predictors_importance &lt;- variable_importance %&gt;%
  as.data.frame() %&gt;%
  rownames_to_column(var=&quot;predictors&quot;)

rf_predictors_importance %&gt;%
  ggplot(aes(x=reorder(predictors, MeanDecreaseAccuracy), y=MeanDecreaseAccuracy)) +
  geom_bar(stat=&quot;identity&quot;) +
  coord_flip() +
  labs(title=&quot;Genres by Importance&quot;, y=&quot;Mean Decrease Accuracy&quot;, x=&quot;Genre&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Observing the graph, it seems that the average ratings for realistic fiction horror, and sci-fi are best able classify users since they show the greatest mean decrease in accuracy on removal.</p>
</div>
<div id="support-vector-machines" class="section level3">
<h3>Support Vector Machines</h3>
<p>The other model we are interested in is a <strong>support vector machine (SVM)</strong> multi-classification model. SVMs work by trying to increase the number of dimensions so it can divide clusters of points using a plane in n-dimensions called a <strong>hyperplane</strong>.</p>
<p>For instance, in the example below, by transforming the points in 2D to 3D, it becomes possible to partition the two clusters of points using a 3D plane whereas this would not have been possible with a line in 2D.</p>
<div class="figure">
<img src="images/svm_example.png" alt="SVM Example: Increasing dimensions allows for a clear separation" />
<p class="caption">SVM Example: Increasing dimensions allows for a clear separation</p>
</div>
<p>Let’s train an SVM model.</p>
<pre class="r"><code>set.seed(1234)

svm_model &lt;- svm(cluster ~ .,
                      data=train_data_ratings, 
                      method=&quot;C-classification&quot;,
                      kernal=&quot;radial&quot;, 
                      gamma=0.1,
                      cost=10)</code></pre>
</div>
<div id="comparing-models" class="section level3">
<h3>Comparing Models</h3>
<p>Now that we have both a random forest model and SVM, it would be useful to do some basic comparisons of between the two. One method of evaluating how good a model is can be done by looking at a <strong>confusion matrix</strong> of the test data. A confusion matrix tabulates the classifications made by a model with the predicted classes as rows and the true classes as columns.</p>
<p>Let’s plot the confusion matrices for our random forest and SVM model on the test data.</p>
<pre class="r"><code>pred_rf &lt;- predict(rf_model, test_data_ratings)
table(pred_rf, test_data_ratings$cluster)</code></pre>
<pre><code>##        
## pred_rf   1   2   3   4   5   6
##       1  16   0   0   0   0   0
##       2   1 457   4  13  21   2
##       3   0   0   7   0   0   0
##       4   1   9   0 220   1   0
##       5   3  13   3   0 383  11
##       6   1   0   1   0   7  76</code></pre>
<pre class="r"><code>pred_svm &lt;- predict(svm_model, test_data_ratings)
table(pred_svm, test_data_ratings$cluster)</code></pre>
<pre><code>##         
## pred_svm   1   2   3   4   5   6
##        1  22   0   0   0   0   0
##        2   0 477   0   1   1   0
##        3   0   0  14   0   0   0
##        4   0   1   0 232   0   0
##        5   0   1   0   0 410   1
##        6   0   0   1   0   1  88</code></pre>
<p>Cursory observation of both matrices indicates that the SVM seems to display slightly higher accuracy over the random forest. Overall, however, both models display very high accuracy, indicating that both the random forest and SVM we trained are very adept at figuring out a user’s cluster based on their average genre ratings.</p>
</div>
</div>
<div id="results-and-discussion" class="section level2">
<h2>Results and Discussion</h2>
<div id="clustering" class="section level3">
<h3>Clustering</h3>
<p>Observing the graph of clusters, groups 2, 4, 5, and 6 appear to be located close to each other. Among the clusters, groups 1, 3, and 6 show the largest amounts of variation within their clusters while groups 2, 4, and 5 are tightly clustered.</p>
<p>The close proximity of groups 2, 4, 5, and 6 to each other likely means that the user ratings in these groups are harder to distinguish from each other which makes sense as these are all groups that are similar to each other in that they have users who have constant average ratings across all genres.</p>
<p>Observing group 6, both its large variation and slight intersection into group 5 seem to indicate that while the average ratings are around 3, there are probably a substantial amount of users within the group that rate very low, dragging the averages down.</p>
<p>Among the clusters that had more specific qualitative descriptions, group 3 has very large variation within its cluster. Moreover, informal observation of the cluster graph reveals smaller clusters within this group as well. This seems to indicate that while the users in this group generally have the qualitative characteristics as described above in common, there are still sub-groups within this cluster that have varying defining characteristics from each other.</p>
<p>Group 1 is another cluster with a more specific qualitative description. The variation within this cluster is also somewhat high and the cluster is located far away from the other groups. This seems to indicate that while there truly is a common group of readers that do not read realistic fiction, they are also likely very different from each other in their average ratings across other genres, despite being in the same cluster.</p>
</div>
<div id="predictions" class="section level3">
<h3>Predictions</h3>
<p>By training our random forest model, we determined that realistic fiction, horror, and sci-fi genre ratings significantly affected which cluster a given user would fall in. This result makes sense when considering how our clusters were separated. While most of the K-means clusters generated seemed to depend less on the value of average user ratings within specific genres and more on a user’s overall average ratings across all genres, certain groups showed exceptions to this. For instance, group 1 seems to made up of users that don’t read or review realistic fiction books and group 3 seems to be made up of users that dislike science fiction and don’t review horror books. The genres in these conditions correspond with 3 of the top 4 most important predictors determined by our random forest model, showing that that the model was able to learn patterns in clustering.</p>
<p>Observing the performance between the random forest and SVM models, the SVM appeared to have slightly outperformed the random forest. Interestingly, the fact that both models performed very well on the test data seems to imply that the random forest and SVM models were able to learn how to determine a user’s cluster based on their average genre ratings alone without needing any underlying context about how the K-means clustering algorithm functioned, showing how powerful and flexible ML models can be at solving problems in general.</p>
</div>
<div id="areas-of-improvement" class="section level3">
<h3>Areas of Improvement</h3>
<p>While we were able to successfully cluster users in this project, there are still several potential areas for improvement. One issue was that by only working with the average genre ratings for each user, we lost information about the variance of ratings within each genre per user. This means that certain users may have ended up in the same cluster because they had similar average ratings but still rate books differently in reality because the variation in how they rate was not accounted for.</p>
<p>Another area that could have been improved was our clustering algorithm. K-means clustering was not able to find some of the smaller clusters in group 6, possibly due to a large amount of users in other locations which ended up occupying the algorithm. Since there were more data points densely packed in the area near groups 2 and 5, the centroids for K-means ended up gathering near there and ignoring smaller clusters elsewhere.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this project, we demonstrated how data from Goodreads could be used to find common groups of users among the site’s readership and predict which groups certain users fall into based on their average genre ratings. We were able to do this through a combination of data manipulation with pipelines, statistical analyses with K-means clustering and PCA, and machine learning with random forests and SVMs.</p>
<p>The clusterings revealed some insights, such as the existence of a large group of users that do not read realistic fiction and a large group of users that have little interest in sci-fi and horror. Meanwhile, the high accuracy of our random forest and SVM were able to show how ML models were able to learn cluster assignment for users without needing any information about the underlying clustering algorithm. While the project as a whole was successful, there are still several areas of improvement we could have addressed for characterizing users, such as accounting for rating variation per user and the use of more advanced clustering algorithms.</p>
<p>Clustering a site’s userbase remains as an important problem that extends beyond Goodreads. In general, being able to find natural clusterings of users is both useful and powerful in that it allows for a large number of people to be bucketed into human-understandable categories. Knowing these clusters can enable platforms to gain a better sense of their userbase and allow them to better target content and appropriately address their communities.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<div class="pixel">

</div>
<div class="holo">

</div>
<div id="refs" class="references">
<div id="ref-kmeansUCR">
<p>Boehmke, Bradley. n.d. “K-Means Cluster Analysis.” <em>UC Business Analytics R Programming Guide</em>.</p>
</div>
<div id="ref-kmeansTDS">
<p>Dabbura, Imad. 2018. “Applications, Evaluation Methods, and Drawbacks.” <em>Towards Data Science</em>.</p>
</div>
<div id="ref-goodbooks2017">
<p>Zajac, Zygmunt. 2017. “Goodbooks-10k: A New Dataset for Book Recommendations.” <em>FastML</em>.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
