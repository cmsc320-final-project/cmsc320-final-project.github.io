<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Analyzing Goodreads’ Readership</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/paper.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 64px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h2 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h3 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h4 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h5 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h6 {
  padding-top: 69px;
  margin-top: -69px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="/">← Back to Home</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Analyzing Goodreads’ Readership</h1>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><a href="https://www.goodreads.com">Goodreads</a> is a popular social cataloging website where users can search, track, and rate books. The site also offers a variety of recommendation services that provide users a way to find books relevant to their reading history. Since its foundation in 2006, the site’s userbase has grown to 20 million users with over 50 million book reviews, making it a very large <a href="https://github.com/zygmuntz/goodbooks-10k">source of readership data</a>.</p>
<div class="figure">
<img src="images/screenshot_01.png" alt="Goodreads site" />
<p class="caption">Goodreads site</p>
</div>
<p>With so many users and reviews, an interesting experiment would be to see if we can characterize the readership of Goodreads based on user ratings. Specifically we are interested in answering the question, “Based on user ratings data, can we find common groups of users by how they rate certain genres of books?” With such information, we would be able to see if, for instance, there is a large group of sci-fi fans or a large group of romance and young adult fanatics that also happen to hate horror.</p>
<blockquote>
<p>“Based on user ratings data, can we find common groups of users by how they rate certain genres of books?”</p>
</blockquote>
<p>In this project, we demonstrate the process of doing this kind of analysis from the ground up by leveraging and manipulating a subset of Goodreads’ data using the <a href="https://github.com/zygmuntz/goodbooks-10k">Goodbooks-10k dataset</a> <span class="citation">(Zajac 2017)</span> and performing statistical and <a href="https://www.statisticssolutions.com/directory-of-statistical-analyses-cluster-analysis/">cluster analysis</a> to try to try to paint a generalized picture of the site’s readership.</p>
</div>
<div id="understanding-and-pre-processing-the-data" class="section level2">
<h2>Understanding and Pre-processing the Data</h2>
<p>The Goodbooks-10k data obtained from <a href="https://github.com/zygmuntz/goodbooks-10k">GitHub</a> consists of several CSV files with information about the top 10,000 most rated books on Goodreads, including book metadata, tags, and relevant user ratings.</p>
<p>The CSV files can be easily loaded using R’s <code>read.csv</code> method. For example, here we load and display <code>books.csv</code> to get a broad picture of one of the datasets we will be working with. We can see that <code>books.csv</code> organizes its data in a tabulated manner with each book as a row and each book attribute as a column.</p>
<pre class="r"><code>books_df &lt;- read.csv(&quot;./goodbooks-10k/books.csv&quot;) %&gt;%
  as_tibble()
head(books_df) %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["book_id"],"name":[1],"type":["int"],"align":["right"]},{"label":["goodreads_book_id"],"name":[2],"type":["int"],"align":["right"]},{"label":["best_book_id"],"name":[3],"type":["int"],"align":["right"]},{"label":["work_id"],"name":[4],"type":["int"],"align":["right"]},{"label":["books_count"],"name":[5],"type":["int"],"align":["right"]},{"label":["isbn"],"name":[6],"type":["fctr"],"align":["left"]},{"label":["isbn13"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["authors"],"name":[8],"type":["fctr"],"align":["left"]},{"label":["original_publication_year"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["original_title"],"name":[10],"type":["fctr"],"align":["left"]},{"label":["title"],"name":[11],"type":["fctr"],"align":["left"]},{"label":["language_code"],"name":[12],"type":["fctr"],"align":["left"]},{"label":["average_rating"],"name":[13],"type":["dbl"],"align":["right"]},{"label":["ratings_count"],"name":[14],"type":["int"],"align":["right"]},{"label":["work_ratings_count"],"name":[15],"type":["int"],"align":["right"]},{"label":["work_text_reviews_count"],"name":[16],"type":["int"],"align":["right"]},{"label":["ratings_1"],"name":[17],"type":["int"],"align":["right"]},{"label":["ratings_2"],"name":[18],"type":["int"],"align":["right"]},{"label":["ratings_3"],"name":[19],"type":["int"],"align":["right"]},{"label":["ratings_4"],"name":[20],"type":["int"],"align":["right"]},{"label":["ratings_5"],"name":[21],"type":["int"],"align":["right"]},{"label":["image_url"],"name":[22],"type":["fctr"],"align":["left"]},{"label":["small_image_url"],"name":[23],"type":["fctr"],"align":["left"]}],"data":[{"1":"1","2":"2767052","3":"2767052","4":"2792775","5":"272","6":"439023483","7":"9.780439e+12","8":"Suzanne Collins","9":"2008","10":"The Hunger Games","11":"The Hunger Games (The Hunger Games, #1)","12":"eng","13":"4.34","14":"4780653","15":"4942365","16":"155254","17":"66715","18":"127936","19":"560092","20":"1481305","21":"2706317","22":"https://images.gr-assets.com/books/1447303603m/2767052.jpg","23":"https://images.gr-assets.com/books/1447303603s/2767052.jpg"},{"1":"2","2":"3","3":"3","4":"4640799","5":"491","6":"439554934","7":"9.780440e+12","8":"J.K. Rowling, Mary GrandPrÃ©","9":"1997","10":"Harry Potter and the Philosopher's Stone","11":"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)","12":"eng","13":"4.44","14":"4602479","15":"4800065","16":"75867","17":"75504","18":"101676","19":"455024","20":"1156318","21":"3011543","22":"https://images.gr-assets.com/books/1474154022m/3.jpg","23":"https://images.gr-assets.com/books/1474154022s/3.jpg"},{"1":"3","2":"41865","3":"41865","4":"3212258","5":"226","6":"316015849","7":"9.780316e+12","8":"Stephenie Meyer","9":"2005","10":"Twilight","11":"Twilight (Twilight, #1)","12":"en-US","13":"3.57","14":"3866839","15":"3916824","16":"95009","17":"456191","18":"436802","19":"793319","20":"875073","21":"1355439","22":"https://images.gr-assets.com/books/1361039443m/41865.jpg","23":"https://images.gr-assets.com/books/1361039443s/41865.jpg"},{"1":"4","2":"2657","3":"2657","4":"3275794","5":"487","6":"61120081","7":"9.780061e+12","8":"Harper Lee","9":"1960","10":"To Kill a Mockingbird","11":"To Kill a Mockingbird","12":"eng","13":"4.25","14":"3198671","15":"3340896","16":"72586","17":"60427","18":"117415","19":"446835","20":"1001952","21":"1714267","22":"https://images.gr-assets.com/books/1361975680m/2657.jpg","23":"https://images.gr-assets.com/books/1361975680s/2657.jpg"},{"1":"5","2":"4671","3":"4671","4":"245494","5":"1356","6":"743273567","7":"9.780743e+12","8":"F. Scott Fitzgerald","9":"1925","10":"The Great Gatsby","11":"The Great Gatsby","12":"eng","13":"3.89","14":"2683664","15":"2773745","16":"51992","17":"86236","18":"197621","19":"606158","20":"936012","21":"947718","22":"https://images.gr-assets.com/books/1490528560m/4671.jpg","23":"https://images.gr-assets.com/books/1490528560s/4671.jpg"},{"1":"6","2":"11870085","3":"11870085","4":"16827462","5":"226","6":"525478817","7":"9.780525e+12","8":"John Green","9":"2012","10":"The Fault in Our Stars","11":"The Fault in Our Stars","12":"eng","13":"4.26","14":"2346404","15":"2478609","16":"140739","17":"47994","18":"92723","19":"327550","20":"698471","21":"1311871","22":"https://images.gr-assets.com/books/1360206420m/11870085.jpg","23":"https://images.gr-assets.com/books/1360206420s/11870085.jpg"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Let’s load the rest of our data as well.</p>
<pre class="r"><code>ratings_df &lt;- read.csv(&quot;./goodbooks-10k/ratings.csv&quot;)%&gt;%
  as_tibble()
tags_df &lt;- read.csv(&quot;./goodbooks-10k/tags.csv&quot;)%&gt;%
  as_tibble()
book_tags_df &lt;- read.csv(&quot;./goodbooks-10k/book_tags.csv&quot;)%&gt;%
  as_tibble()</code></pre>
<p>With our data loaded, we can begin to do some data manipulation. Since we are interested in analyzing various genres, let’s start by narrowing down the tags dataframe, <code>tags_df</code>, with the genre tags we are interested in. We can do this using a <a href="https://cfss.uchicago.edu/notes/pipes/"><strong>pipeline</strong></a> as shown below.</p>
<pre class="r"><code>tags_df &lt;- tags_df %&gt;%
  filter(tag_name %in% c(
    &quot;fantasy&quot;,
    &quot;mystery&quot;,
    &quot;horror&quot;,
    &quot;realistic-fiction&quot;,
    &quot;romance&quot;,
    &quot;science-fiction&quot;,
    &quot;thriller&quot;,
    &quot;young-adult&quot;
  )) %&gt;%
  as_tibble()

tags_df %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["tag_id"],"name":[1],"type":["int"],"align":["right"]},{"label":["tag_name"],"name":[2],"type":["fctr"],"align":["left"]}],"data":[{"1":"11305","2":"fantasy"},{"1":"14821","2":"horror"},{"1":"20939","2":"mystery"},{"1":"25438","2":"realistic-fiction"},{"1":"26138","2":"romance"},{"1":"26837","2":"science-fiction"},{"1":"30358","2":"thriller"},{"1":"33114","2":"young-adult"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>However, this tag information is not very helpful by itself. We want to be able to use it in conjunction with the other data we have. One way to accomplish this is by <a href="http://www.datasciencemadesimple.com/join-in-r-merge-in-r/"><strong>joining</strong></a> data across different data frames through a common attribute. We construct a pipeline to link data from <code>books_df</code> and <code>book_tags_df</code> through the shared attribute, <code>goodreads_book_id</code>. From there, we then link <code>book_tags_df</code> and <code>tags_df</code> through <code>tag_id</code>, creating a new dataframe with an entry for each book’s genre tag.</p>
<pre class="r"><code>book_genres_df &lt;- books_df %&gt;%
  inner_join(book_tags_df, by=&quot;goodreads_book_id&quot;) %&gt;%
  inner_join(tags_df, by=&quot;tag_id&quot;) %&gt;%
  select(&quot;book_id&quot;, &quot;title&quot;, &quot;tag_name&quot;)

book_genres_df %&gt;%
  select(&quot;title&quot;, &quot;tag_name&quot;) %&gt;%
  head(15) %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["title"],"name":[1],"type":["fctr"],"align":["left"]},{"label":["tag_name"],"name":[2],"type":["fctr"],"align":["left"]}],"data":[{"1":"The Hunger Games (The Hunger Games, #1)","2":"young-adult"},{"1":"The Hunger Games (The Hunger Games, #1)","2":"fantasy"},{"1":"The Hunger Games (The Hunger Games, #1)","2":"science-fiction"},{"1":"The Hunger Games (The Hunger Games, #1)","2":"romance"},{"1":"The Hunger Games (The Hunger Games, #1)","2":"thriller"},{"1":"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)","2":"fantasy"},{"1":"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)","2":"young-adult"},{"1":"Harry Potter and the Sorcerer's Stone (Harry Potter, #1)","2":"mystery"},{"1":"Twilight (Twilight, #1)","2":"young-adult"},{"1":"Twilight (Twilight, #1)","2":"fantasy"},{"1":"Twilight (Twilight, #1)","2":"horror"},{"1":"Twilight (Twilight, #1)","2":"science-fiction"},{"1":"To Kill a Mockingbird","2":"young-adult"},{"1":"To Kill a Mockingbird","2":"realistic-fiction"},{"1":"To Kill a Mockingbird","2":"mystery"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>From the resulting data frame, we can see that <code>tag_name</code> does indeed describe the genre of books it is associated with.</p>
<p>In order to answer our question, we need to collate data on how users review books from certain genres. To do so, let’s build another pipeline to join with <code>ratings_df</code> and link users with the genres of the books they rated. We can do a summary on the data frame from there to end up with a table of mean ratings in each genre of interest for all users. These ratings range from 1 to 5, with a mean rating of -5 to mark genres with no rating.</p>
<pre class="r"><code># Note: This takes awhile to run...

user_features_raw_df &lt;- ratings_df %&gt;%
  inner_join(book_genres_df, by=&quot;book_id&quot;) %&gt;%
  spread(tag_name, rating) %&gt;%
  select(-book_id, -title,)

user_features_df &lt;- user_features_raw_df %&gt;%
  group_by(user_id) %&gt;%
  summarize_all(mean, na.rm=TRUE) %&gt;%
  select(-user_id,) %&gt;%
  mutate_if(~ any(is.na(.x)),~ if_else(is.na(.x),-5,.x)) # Replace na with -5

# Convert column names to snake_case
colnames(user_features_df) = to_snake_case(colnames(user_features_df))

head(user_features_df, 15) %&gt;%
  mutate_all(round, 2) %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["fantasy"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["horror"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["mystery"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["realistic_fiction"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["romance"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["science_fiction"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["thriller"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["young_adult"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"3.55","2":"3.29","3":"3.40","4":"3.51","5":"3.45","6":"3.14","7":"3.40","8":"3.68"},{"1":"4.85","2":"5.00","3":"4.90","4":"4.38","5":"4.12","6":"4.75","7":"4.67","8":"4.76"},{"1":"1.45","2":"1.50","3":"1.24","4":"2.00","5":"1.76","6":"1.17","7":"1.13","8":"1.83"},{"1":"3.90","2":"3.59","3":"3.55","4":"3.65","5":"3.44","6":"3.54","7":"3.18","8":"4.08"},{"1":"4.11","2":"4.03","3":"4.03","4":"3.95","5":"3.93","6":"4.11","7":"4.00","8":"3.95"},{"1":"4.20","2":"4.25","3":"4.14","4":"4.29","5":"4.43","6":"4.17","7":"4.25","8":"4.42"},{"1":"3.79","2":"3.78","3":"3.76","4":"4.00","5":"3.51","6":"3.76","7":"3.60","8":"3.88"},{"1":"4.09","2":"3.95","3":"3.81","4":"3.25","5":"3.17","6":"4.42","7":"3.40","8":"3.50"},{"1":"3.66","2":"3.67","3":"3.25","4":"3.37","5":"3.52","6":"3.58","7":"3.03","8":"3.63"},{"1":"4.02","2":"3.88","3":"3.92","4":"3.86","5":"3.76","6":"3.89","7":"3.96","8":"4.07"},{"1":"3.63","2":"3.69","3":"3.42","4":"3.59","5":"3.44","6":"3.40","7":"3.69","8":"3.93"},{"1":"3.54","2":"3.38","3":"3.85","4":"3.86","5":"3.56","6":"3.81","7":"3.93","8":"3.65"},{"1":"3.64","2":"4.00","3":"4.33","4":"4.29","5":"4.11","6":"4.00","7":"4.40","8":"4.24"},{"1":"3.61","2":"3.60","3":"3.60","4":"3.08","5":"3.32","6":"3.66","7":"3.31","8":"3.34"},{"1":"4.26","2":"4.45","3":"4.00","4":"4.35","5":"4.31","6":"4.33","7":"4.08","8":"4.38"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
<div id="visualization-and-analysis" class="section level2">
<h2>Visualization and Analysis</h2>
<p>Now that we have transformed the original data into a data frame of average genre ratings for all users, we can begin analyzing users based on genre ratings.</p>
<p>One way to better understand data in general is through visualization. For example, if we want to see where the readership lies on ratings for young adult versus fantasy, we can do so by plotting the average ratings from both genres on a 2D scatterplot.</p>
<pre class="r"><code>ggplot(user_features_df, mapping=aes(x=fantasy, y=young_adult)) + 
# If you don&#39;t want to see user averages for users who have not rated books from either genre, uncomment the next two lines
#  scale_x_continuous(limits = c(0, 5)) + 
#  scale_y_continuous(limits = c(0, 5)) + 
  geom_point() +
  labs(
    title=&quot;User Average Ratings for Young Adult Versus Fantasy&quot;,
    x=&quot;Fantasy Rating&quot;,
    y=&quot;Young Adult Ratings&quot;
  )</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>From the graph, it appears most user have average ratings clustered between 3 and 5 with no strong preference for one genre versus the other. There are also several users that appear to be readers of one genre but not the other, and at least one user that does not read from either.</p>
<div id="clustering-for-higher-dimensions" class="section level3">
<h3>Clustering for Higher Dimensions</h3>
<p>While a 2D graph is fine for comparing two genres, it becomes harder to visualize data like this beyond 3 variables since we run out of spatial dimensions. Furthermore, we are also interested in clustering our data. While we had done a very cursory grouping by eye-balling the data above, ideally, there should be a way to more formally say which data points form a group with one another.</p>
<p>The solution to both of these problems is found in two methods known as <a href="https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1"><strong>K-means clustering</strong></a> and <a href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c"><strong>principal component analysis (PCA)</strong></a>.</p>
<p>K-means clustering is a way to cluster, or group, data points. The algorithm designates K centroids representing the centers of their respective clusters. Each data point is assigned to the closest centroid, and the algorithm iteratively optimizes the centroids to try to reduce the total distance between it and all the points in its cluster.</p>
<div class="figure">
<img src="images/kmeans_iris.png" alt="K-means on iris flower species" />
<p class="caption">K-means on iris flower species</p>
</div>
<p>Meanwhile, principal component analysis (PCA) is a way to reduce the dimensions in high-dimensional data while still trying to preserve features from the original space. Roughly, PCA tries to find a set of linear transformations from the original data that prioritizes maximizing variance for each dimension of the transformed data. At the end, the first two dimensions of the transformed data will ideally encode enough of the variance from the original high-dimensional data that graphing only those two dimensions should suffice in representing the original data, making PCA a very useful tool for visualization.</p>
<div class="figure">
<img src="images/pcafish.png" alt="Example PCA: The transformed coordinate system in red maximizes the variance of the pixel locations." />
<p class="caption">Example PCA: The transformed coordinate system in red maximizes the variance of the pixel locations.</p>
</div>
<p>When used in conjunction, K-means clustering will allow us to formally cluster multi-dimensional data and PCA will allow us graph the clustered data in 2D for easy visualization.</p>
</div>
<div id="determining-k" class="section level3">
<h3>Determining K</h3>
<p>A key roadblock with using K-means is that we do not have a good idea of what is a “good” K, or number of clusters. One way we can address this is by doing several test runs with different K’s and comparing them, specifically, by comparing their total <strong>sum of squared distances (SSE)</strong>. Intuitively, a good clustering should minimize the distances of points to the centroids of their respective clusters which would be reflected in a low total SSE <span class="citation">(Dabbura 2018)</span>.</p>
<p>While we want to lower variation within clusters, at the same time, we also want our clusters to actually be clusters. This means that for any data point, it should ideally be close to the centroid of its cluster and far from others, a metric known as the <strong>silhouette</strong> <span class="citation">(Boehmke, n.d.)</span>. While increasing K to an unreasonably large number may reduce total SSE, data in such a clustering would have low silhouettes since data points in one cluster would be very close to data points in other clusters.</p>
<p>The goal of finding a good K then resides in reducing SSE while still trying to keep silhouette high. Let’s compute and graph both SSE and silhouette from a sample of the users to see what a good K might be.</p>
<pre class="r"><code>set.seed(320)

k_range &lt;- 2:20
user_features_sample_df &lt;- sample_n(user_features_df, 1000)

# SSE
sse &lt;- sapply(k_range, function(k) {
  kmeans(user_features_sample_df, k)$tot.withinss
})

ggplot(mapping=aes(x=k_range, y=sse)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title=&quot;Total SSE vs K&quot;,
    x=&quot;k&quot;,
    y=&quot;Total SSE&quot;
  )</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Average silhouettes
sil &lt;- sapply(k_range, function(k) {
  km.res &lt;- kmeans(user_features_sample_df, centers=k)
  ss &lt;- silhouette(km.res$cluster, dist(user_features_sample_df))
  mean(ss[,3])
})

ggplot(mapping=aes(x=k_range, y=sil)) +
  geom_line() +
  geom_label(aes(label=k_range)) + 
  labs(
    title=&quot;Average Silhouettes vs K&quot;,
    x=&quot;k&quot;,
    y=&quot;Average Silhouette&quot;
  )</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>Observing both graphs, K = 6 appears to be a point where total SSE has begun to flatten but average silhouette is still somewhat high, making it a reasonable compromise for K. Above K = 6, the total SSE does decrease, but not significantly enough to warrant a corresponding decrease in average silhouette. With K in hand, we can run K-means clustering on the full data now and graph using PCA.</p>
<pre class="r"><code>set.seed(320)

kmeans_res &lt;- kmeans(user_features_df, 6)
kmeans_res$centers %&gt;%
  as_tibble() %&gt;%
  mutate_all(round, 2) %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["fantasy"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["horror"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["mystery"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["realistic_fiction"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["romance"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["science_fiction"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["thriller"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["young_adult"],"name":[8],"type":["dbl"],"align":["right"]}],"data":[{"1":"4.08","2":"4.06","3":"4.09","4":"-5.00","5":"4.03","6":"3.99","7":"4.00","8":"4.03"},{"1":"4.07","2":"3.95","3":"4.03","4":"3.96","5":"3.98","6":"4.04","7":"3.99","8":"4.07"},{"1":"3.54","2":"-5.00","3":"3.99","4":"3.88","5":"3.96","6":"1.90","7":"3.13","8":"4.09"},{"1":"4.57","2":"4.53","3":"4.56","4":"4.48","5":"4.52","6":"4.56","7":"4.55","8":"4.56"},{"1":"3.62","2":"3.45","3":"3.58","4":"3.54","5":"3.54","6":"3.57","7":"3.53","8":"3.63"},{"1":"3.11","2":"2.96","3":"3.11","4":"3.13","5":"3.10","6":"2.59","7":"2.69","8":"3.15"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code># PCA
pca_res &lt;- prcomp(user_features_df, center=TRUE, scale.=TRUE)
plot_data &lt;- pca_res$x[,1:2] %&gt;%
  as_tibble() %&gt;%
  mutate(cluster=factor(kmeans_res$cluster))
 
ggplot(plot_data, aes(PC1, PC2, color=cluster)) +
  geom_point() + 
  labs(title=&quot;Readership Clusters&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="making-predictions" class="section level2">
<h2>Making Predictions</h2>
<p>Here in this section we talk about prediction analysis using R, specifically with regards to basic machine learning. In the following code chunk, we take our previous results from k-means clustering, and then append them to our previous dataframe in order to augment it into a <strong>training data frame</strong>. Notice also here that we’re also generating the individual Principal Components used in PCA before. The intent here is that we can select a subset of the most important (cover the most variability) Principal Components and use those in other prediction methods. We’ve also included a graph of the PC’s and how they correspond to clustering with the k-means algorithm.</p>
<pre class="r"><code>set.seed(1234)

# Set up training data for cluster prediction   
training_data_ratings &lt;- user_features_df %&gt;%
  mutate(cluster=factor(kmeans_res$cluster)) %&gt;%
  sample_n(5000)

# Setting up training data for PCA anlaysis
training_data_pca &lt;-  as.data.frame(predict(pca_res, training_data_ratings)) %&gt;% as_tibble()
training_data_pca$cluster &lt;- training_data_ratings$cluster

training_data_ratings %&gt;%   
  head(15) %&gt;%  
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["fantasy"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["horror"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["mystery"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["realistic_fiction"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["romance"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["science_fiction"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["thriller"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["young_adult"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["cluster"],"name":[9],"type":["fctr"],"align":["left"]}],"data":[{"1":"4.000000","2":"4.200000","3":"4.137931","4":"4.090909","5":"3.956522","6":"3.833333","7":"3.772727","8":"4.068182","9":"2"},{"1":"4.346154","2":"4.583333","3":"4.258065","4":"4.375000","5":"4.114286","6":"4.388889","7":"4.200000","8":"4.357143","9":"4"},{"1":"2.542373","2":"1.750000","3":"2.866667","4":"3.023810","5":"2.710744","6":"2.555556","7":"4.000000","8":"2.764706","9":"6"},{"1":"3.250000","2":"3.208333","3":"3.277778","4":"3.210526","5":"3.470588","6":"3.263158","7":"3.355769","8":"3.531250","9":"5"},{"1":"4.294118","2":"4.000000","3":"3.954545","4":"4.090909","5":"4.026316","6":"4.200000","7":"3.823529","8":"4.388889","9":"2"},{"1":"3.611765","2":"3.611111","3":"3.478261","4":"3.266667","5":"3.238095","6":"3.606061","7":"3.350000","8":"3.512821","9":"5"},{"1":"3.923077","2":"3.666667","3":"4.102564","4":"4.215385","5":"4.146667","6":"3.736842","7":"3.800000","8":"4.115789","9":"2"},{"1":"3.642857","2":"3.375000","3":"3.620000","4":"3.517241","5":"3.230769","6":"3.500000","7":"3.589744","8":"4.066667","9":"5"},{"1":"3.400000","2":"3.333333","3":"3.148936","4":"3.279070","5":"3.283582","6":"3.416667","7":"2.892857","8":"3.333333","9":"5"},{"1":"4.392157","2":"4.080000","3":"4.159091","4":"4.333333","5":"4.166667","6":"4.421053","7":"4.148936","8":"4.375000","9":"2"},{"1":"4.323944","2":"3.857143","3":"4.432432","4":"4.066667","5":"4.509091","6":"4.300000","7":"4.426471","8":"4.555556","9":"4"},{"1":"4.075758","2":"4.000000","3":"3.861111","4":"4.000000","5":"3.894737","6":"4.100000","7":"4.111111","8":"4.000000","9":"2"},{"1":"3.419355","2":"3.437500","3":"3.433333","4":"3.700000","5":"3.357143","6":"3.418182","7":"3.378788","8":"3.400000","9":"5"},{"1":"2.428571","2":"3.000000","3":"2.571429","4":"3.250000","5":"3.066667","6":"2.000000","7":"3.000000","8":"2.904762","9":"6"},{"1":"4.543478","2":"4.666667","3":"4.516667","4":"4.608696","5":"4.189189","6":"4.516129","7":"4.509091","8":"4.487179","9":"4"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>training_data_pca %&gt;%
  head(15) %&gt;%
  rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["PC1"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["PC2"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["PC3"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["PC4"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["PC5"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["PC6"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["PC7"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["PC8"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["cluster"],"name":[9],"type":["fctr"],"align":["left"]}],"data":[{"1":"0.5432715","2":"0.06760501","3":"-0.26117119","4":"0.049408924","5":"0.35661962","6":"-0.0720484189","7":"-0.15743328","8":"0.209150837","9":"2"},{"1":"1.7124988","2":"-0.09532340","3":"-0.41587198","4":"0.001793298","5":"0.13933396","6":"0.0741115408","7":"0.12240584","8":"0.168393079","9":"4"},{"1":"-4.4858635","2":"0.29207600","3":"0.19098462","4":"1.319743745","5":"-1.31901999","6":"0.2895388994","7":"0.12493279","8":"0.002011214","9":"6"},{"1":"-2.1683099","2":"-0.07469882","3":"-0.06015029","4":"0.131769337","5":"0.01353766","6":"0.3659025899","7":"-0.07502277","8":"-0.238329350","9":"5"},{"1":"1.0042900","2":"0.07549336","3":"-0.19354034","4":"-0.444154133","5":"0.20624783","6":"0.2619933142","7":"0.37055970","8":"-0.025279148","9":"2"},{"1":"-1.7376314","2":"-0.46859041","3":"-0.33917834","4":"-0.062124867","5":"-0.08948850","6":"-0.0689205683","7":"0.23463564","8":"0.233711202","9":"5"},{"1":"0.5289511","2":"0.51379435","3":"-0.01537821","4":"-0.009720388","5":"0.24764651","6":"0.0582680245","7":"-0.29587416","8":"-0.057938315","9":"2"},{"1":"-1.1468758","2":"0.03331248","3":"-0.07732837","4":"-0.057802762","5":"0.02696398","6":"0.5232854352","7":"0.55128740","8":"0.637532659","9":"5"},{"1":"-2.5592775","2":"-0.31325136","3":"-0.47189964","4":"-0.327458115","5":"0.06364221","6":"0.0008810543","7":"0.07727219","8":"-0.222155851","9":"5"},{"1":"1.5987138","2":"0.11897876","3":"-0.21761659","4":"-0.244206519","5":"-0.04598309","6":"0.1092285365","7":"0.21672458","8":"-0.023056364","9":"2"},{"1":"2.1594241","2":"0.36319724","3":"0.42779839","4":"-0.032419488","5":"0.02760074","6":"0.2542841936","7":"-0.17024490","8":"-0.082556828","9":"4"},{"1":"0.4501249","2":"-0.12269423","3":"-0.28559652","4":"0.122082956","5":"-0.17601531","6":"0.0715410461","7":"0.22023329","8":"-0.121412455","9":"2"},{"1":"-1.9275514","2":"-0.07091472","3":"-0.48955500","4":"0.167066369","5":"-0.12405981","6":"-0.0504020783","7":"-0.03436655","8":"0.004469251","9":"5"},{"1":"-4.7187872","2":"0.20184691","3":"-0.44044534","4":"0.971369577","5":"0.35026165","6":"0.4968258379","7":"-0.15497733","8":"-0.632476016","9":"6"},{"1":"2.4385145","2":"0.01557281","3":"-0.36082437","4":"0.137893680","5":"0.02324007","6":"-0.0499055453","7":"0.17861266","8":"0.370130036","9":"4"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div id="random-forests" class="section level3">
<h3>Random Forests</h3>
<p>Now that we’ve generated our training data set, we can begin training our own machine learning (ML) model. First, we’ll take a look at a popular tree-based model known as a <a href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2"><strong>random forest</strong></a>. In a regular decision tree, the idea is to break the classification problem of ML up into multiple partitions first, and this forms the basis of the actual tree. With random forests, we take this a step further by creating multiple, random decision trees and then averaging the results from each of them for the classification problem. This improves prediction and also reduces instability from just singular decision trees.</p>
<pre class="r"><code># Training classification on dataframe using randomForest (n = 200):    
rf_prediction &lt;- randomForest(cluster ~ .,  
                              ntree=200,    
                              importance=TRUE,  
                              data=training_data_ratings)   </code></pre>
<p>Now that we’ve generated our random forest model using all of our elements/predictors (the ratings of each genre) and sampling 200 decision trees, we can now take a look at the actual importance of each predictor. The importance of each predictor is judged by the <strong>Mean Decrease in Accuracy</strong>, which is calculated as the decrease in accuracy when the predictor is removed from the model, allowing us to see which predictors are the most influential. When comparing mean decrease in accuracy, a larger mean decrease for one predictor over another means that if the first predictor were removed, it would result in the model making on average more misclassifications than the second.</p>
<pre class="r"><code>variable_importance &lt;- importance(rf_prediction)

rf_predictors_importance &lt;- variable_importance %&gt;%
  as.data.frame() %&gt;%
  rownames_to_column(var=&quot;predictors&quot;)

rf_predictors_importance %&gt;%
  ggplot(aes(x=reorder(predictors, MeanDecreaseAccuracy), y=MeanDecreaseAccuracy)) +
  geom_bar(stat=&quot;identity&quot;) +
  coord_flip() +
  labs(title=&quot;Genres by Importance&quot;, y=&quot;Mean Decrease Accuracy&quot;, x=&quot;Genre&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>From the graph, it seems that ratings in certain genres such as realistic fiction and horror are able to better classify users by reducing error in classifying.</p>
</div>
<div id="support-vector-machines" class="section level3">
<h3>Support Vector Machines</h3>
<p>As an additional example, we’ll also show how to train using a <strong>support vector machine</strong> (SVM) multi-classification model as an additional factor in our experiment below. The way SVM works, is to actually <strong>increase dimension</strong> as compared to PCA. Basically, we want to create a hyperplane - some n-dimensional vector that can effectively separate between multiple classes in the classification problem. As an example of how this work, consider a simple 2 group classification problem in 2D:</p>
<div class="figure">
<img src="images/svm_example.png" alt="Example SVM: Moving from 2D coordinate space to 3D in order to better separate classification" />
<p class="caption">Example SVM: Moving from 2D coordinate space to 3D in order to better separate classification</p>
</div>
<p>In the example above, the 2D plot is circular, meaning there is no easy, linear division between the red and blue points. However, if we extend to 3D (by letting <span class="math inline">\(z = x^2 + y^2\)</span>) we get a shape in 3D that we can divide using a plane. Notice that by translating this plane into 2D, we actually get a circle although in 3D it is “linear/planar”. The “support vectors” in SVM refer to the data points closest to the hyperplane, which are used to help determine the hyperplane itself. In the next example, we use the ratings training data set to train our SVM model:</p>
<pre class="r"><code>set.seed(1234)    # Setting seed for reproducibility

n &lt;- nrow(training_data_ratings)  # Number of observations
ntrain &lt;- round(n*0.75)  # 75% for training set
split_index &lt;- sample(n, ntrain)   # Create a random index
train_svm &lt;- training_data_ratings[split_index,]   # Create training set
test_svm &lt;- training_data_ratings[-split_index,]   # Create test set

svm_prediction &lt;- svm(cluster ~ .,
                      data=train_svm, 
                      method=&quot;C-classification&quot;,
                      kernal=&quot;radial&quot;, 
                      gamma=0.1,
                      cost=10) #coefficients determined w/ help from guide
#summary of SVM model stats:
summary(svm_prediction)</code></pre>
<pre><code>## 
## Call:
## svm(formula = cluster ~ ., data = train_svm, method = &quot;C-classification&quot;, 
##     kernal = &quot;radial&quot;, gamma = 0.1, cost = 10)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  10 
## 
## Number of Support Vectors:  459
## 
##  ( 139 150 65 71 23 11 )
## 
## 
## Number of Classes:  6 
## 
## Levels: 
##  1 2 3 4 5 6</code></pre>
<p>Looking at our model, we notice that we used 459 total support vectors, and you can also show from the summary the type of SVM classification we specified (C-Classification, most popularly used) and also the kernel function we used to reduce computing time (radial).</p>
<p>Now we can visualize our hyperplane and the way our SVM model is classifying using an in-built function for SVM. For this, we’ll consider the differences between ratings for realistic fiction and horror (2 predictors of importance according to our previous random forest model) while keeping the others constant:</p>
<pre class="r"><code># using plot() to visualize our hyperplane:
plot(svm_prediction, train_svm, realistic_fiction ~ horror,
          slice=list(fantasy = 3, mystery = 3, young_adult = 3, romance = 3, science_fiction = 3, thriller = 3))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>From the plot above, we keep the predictors for all the other predictors constant (at rating = 3), and are now taking a look at how SVM is plotting a hyperplane between clusters for the two ratings: realistic_fiction and horror. The color gradients in the plot correspond to the clusters our SVM model predicts, and from here you can see the support vectors (the points on the plot) and the margins between the respective clusters.</p>
</div>
<div id="comparing-models" class="section level3">
<h3>Comparing Models</h3>
<pre class="r"><code>pred_svm &lt;- predict(svm_prediction, test_svm)

table(test_svm$cluster, pred_svm)</code></pre>
<pre><code>##    pred_svm
##       1   2   3   4   5   6
##   1  20   0   0   0   0   0
##   2   0 439   0   0   1   0
##   3   0   0  11   0   0   1
##   4   0   1   0 244   0   0
##   5   0   1   0   0 446   3
##   6   0   0   0   0   1  82</code></pre>
<pre class="r"><code>prop.table(table(test_svm$cluster, pred_svm), 1)</code></pre>
<pre><code>##    pred_svm
##               1           2           3           4           5           6
##   1 1.000000000 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000
##   2 0.000000000 0.997727273 0.000000000 0.000000000 0.002272727 0.000000000
##   3 0.000000000 0.000000000 0.916666667 0.000000000 0.000000000 0.083333333
##   4 0.000000000 0.004081633 0.000000000 0.995918367 0.000000000 0.000000000
##   5 0.000000000 0.002222222 0.000000000 0.000000000 0.991111111 0.006666667
##   6 0.000000000 0.000000000 0.000000000 0.000000000 0.012048193 0.987951807</code></pre>
</div>
</div>
<div id="results-and-discussion" class="section level2">
<h2>Results and Discussion</h2>
<div id="clustering" class="section level3">
<h3>Clustering</h3>
<p>Let’s try to qualitatively summarize our clusters:</p>
<ol style="list-style-type: decimal">
<li>Rates average 4 in all genres but doesn’t read realistic fiction</li>
<li>Rates average 4 in all genres</li>
<li>Rates average 3-4 in all genres but dislikes SF and doesn’t read horror</li>
<li>Rates average 4.5-5 in all genres</li>
<li>Rates average 3.5 in all genres</li>
<li>Rates average 3 all genres</li>
</ol>
<p>Observing the graph of clusters, groups 2, 4, 5, and 6 appear to be located close to each other. Among the clusters, groups 1, 3, and 6 show the largest amounts of variation within their clusters while groups 2, 4, and 5 are tightly clustered.</p>
<p>The close locations of groups 2, 4, 5, and 6 mean that the user ratings in these groups are harder to distinguish from each other which makes sense as these are all groups that are similar to each other in that they have users who have constant average ratings across all genres.</p>
<p>Observing group 6, both its large variation and slight intersection into group 5 seem to indicate that while the average ratings are around 3, there are probably a substantial amount of users within the group that rate very low, dragging the averages down.</p>
<p>Group 3 is also an interesting case. The cluster has large variation and a somewhat specific qualitative description. Moreover, informal observation of the cluster graph seems to show smaller clusters within this group as well. This seems to indicate that while the users in this group generally have the qualitative characteristics as described above in common, there are still sub-groups within this cluster that have varying defining characteristics from each other.</p>
<p>Group 1 is another cluster with a specific qualitative description. The variation within this cluster is somewhat high and the cluster is located far away from other the other groups. This seems to indicate that there truly is a common group of readers that do not read realistic fiction, but they also likely vary in their average ratings across other genres.</p>
</div>
<div id="predictions" class="section level3">
<h3>Predictions</h3>
<p>Looking at our predictions, we got some very interesting results. By training our Random Forest model, we were able to show on the regular dataset that the most “important” predictors for classification seemed to be realistic_fiction and horror genre ratings. This result makes sense when considering how our clusters are separated. While most of the K-means clusters we generated seemed to depends less upon average user ratings for specific genres than their average ratings across all genres, cluster 1 seems to made up of users that don’t read or review realistic fiction books and cluster 3 seems to be made up of users that dislike science fiction and don’t review horror books. Those extra genre conditions, realistic fiction, science fiction, and horror correspond with with 3 of the top 4 most important predictors according to our random forest model.</p>
<p><strong>TODO: Add more on SVM</strong></p>
<p>Using our Support Vector Machine model, we achieved very interesting accuracy with our testing data set. Overall, it suggests that perhaps SVM is a good model/fit for the classification problem.</p>
</div>
<div id="areas-of-improvement" class="section level3">
<h3>Areas of Improvement</h3>
<p>While we were able to successfully cluster users in this project, there are still several potential areas for improvement. One issue was that by only working with the average genre ratings for each user, we lost information about the variance of ratings within each genre per user. This means that certain users may have ended up in the same cluster because they had similar average ratings but still rate books differently in reality because the variation in how they rate was not accounted for.</p>
<p>Another area that could have been improved was our clustering algorithm. K-means clustering was not able to find some of the smaller clusters in group 6, possibly due to a large amount of users in other locations which ended up occupying the algorithm. Since there were more data points densely packed in the area near groups 2 and 5, the centroids for K-means ended up gathering near there and ignoring smaller clusters elsewhere.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this project, we demonstrated how data from Goodreads could be used to find common groups of users among the site’s readership. We were able to do this through a combination of data manipulation with pipelines and statistical analyses with K-means clustering and PCA.</p>
<p>These clusterings revealed some insights, such as the existence of a large group of users that do not read realistic fiction and a large group of users that have little interest in SF and horror. While our clustering was successful, there are still several areas of improvement we could have addressed for characterizing users, such as accounting for rating variation per user and the use of more advanced clustering algorithms.</p>
<p>Clustering a site’s userbase is a problem that extends beyond Goodreads. In general, being able to find natural clusterings of users is both useful and powerful in that it allows for a large number of people to be bucketed into human-understandable categories. Knowing these clusters can let platforms gain a better sense of their userbase and allow them to better target content and appropriately address their communities.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<div class="pixel">

</div>
<div class="holo">

</div>
<div id="refs" class="references">
<div id="ref-kmeansUCR">
<p>Boehmke, Bradley. n.d. “K-Means Cluster Analysis.” <em>UC Business Analytics R Programming Guide</em>.</p>
</div>
<div id="ref-kmeansTDS">
<p>Dabbura, Imad. 2018. “Applications, Evaluation Methods, and Drawbacks.” <em>Towards Data Science</em>.</p>
</div>
<div id="ref-goodbooks2017">
<p>Zajac, Zygmunt. 2017. “Goodbooks-10k: A New Dataset for Book Recommendations.” <em>FastML</em>.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
